"""
Evaluation Metrics for The Showdown (Part 4)

Provides:
    - ROUGE-L scoring (precision, recall, F1)
    - LLM-as-a-Judge (faithfulness & accuracy on 1-5 scale)
    - Latency measurement utility
"""

from __future__ import annotations

import json
import re
import statistics
import time
from typing import Any, Callable, Dict, List, Optional

from rouge_score import rouge_scorer


# ---------------------------------------------------------------------------
# ROUGE-L
# ---------------------------------------------------------------------------

_SCORER: Optional[rouge_scorer.RougeScorer] = None


def _get_scorer() -> rouge_scorer.RougeScorer:
    """Lazy-init a shared RougeScorer instance."""
    global _SCORER
    if _SCORER is None:
        _SCORER = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)
    return _SCORER


def compute_rouge_l(prediction: str, reference: str) -> Dict[str, float]:
    """
    Compute ROUGE-L between a predicted answer and a reference answer.

    Returns
    -------
    dict
        ``precision``, ``recall``, ``fmeasure`` – each a float in [0, 1].
    """
    scorer = _get_scorer()
    scores = scorer.score(reference, prediction)
    rl = scores["rougeL"]
    return {
        "precision": round(rl.precision, 4),
        "recall": round(rl.recall, 4),
        "fmeasure": round(rl.fmeasure, 4),
    }


# ---------------------------------------------------------------------------
# LLM-as-a-Judge
# ---------------------------------------------------------------------------

_JUDGE_PROMPT = """You are an expert evaluator for a financial question-answering system.

You will be given:
- A **question** about a company's annual report.
- The **ground truth** answer (the correct reference answer).
- A **predicted answer** generated by the system under evaluation.

Score the predicted answer on two criteria, each on a scale of 1 to 5:

1. **Faithfulness** – Does the predicted answer only contain information that is supported by the ground truth or the context? (1 = hallucinated / contradicts ground truth, 5 = fully faithful)
2. **Accuracy** – How factually correct and complete is the predicted answer compared to the ground truth? (1 = completely wrong, 5 = perfectly matches ground truth)

Respond with ONLY a JSON object in this exact format (no markdown, no extra text):
{{"faithfulness": <int 1-5>, "accuracy": <int 1-5>, "reasoning": "<brief 1-2 sentence justification>"}}

---

**Question:** {question}

**Ground Truth Answer:** {ground_truth}

**Predicted Answer:** {prediction}
"""


def llm_judge_score(
    question: str,
    ground_truth: str,
    prediction: str,
    config: Dict[str, Any],
) -> Dict[str, Any]:
    """
    Use a strong LLM (reasoning model) to score a predicted answer.

    Parameters
    ----------
    question : str
        The original question.
    ground_truth : str
        The correct reference answer.
    prediction : str
        The model's generated answer.
    config : dict
        Full project config (reads ``evaluation.metrics.llm_judge``).

    Returns
    -------
    dict
        ``faithfulness`` (int 1-5), ``accuracy`` (int 1-5), ``reasoning`` (str).
        On failure returns scores of 0 with an error message in reasoning.
    """
    from openai import OpenAI

    judge_cfg = config.get("evaluation", {}).get("metrics", {}).get("llm_judge", {})
    model = judge_cfg.get("model", "o3-mini")

    prompt = _JUDGE_PROMPT.format(
        question=question,
        ground_truth=ground_truth,
        prediction=prediction,
    )

    try:
        client = OpenAI(timeout=90)
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "user", "content": prompt},
            ],
            # o3-mini / reasoning models don't support temperature
        )
        raw = response.choices[0].message.content.strip()

        # Strip markdown fences if the model wraps in ```json ... ```
        if raw.startswith("```"):
            raw = re.sub(r"^```(?:json)?\s*", "", raw)
            raw = re.sub(r"\s*```$", "", raw)

        result = json.loads(raw)

        return {
            "faithfulness": int(result.get("faithfulness", 0)),
            "accuracy": int(result.get("accuracy", 0)),
            "reasoning": str(result.get("reasoning", "")),
        }

    except Exception as e:
        return {
            "faithfulness": 0,
            "accuracy": 0,
            "reasoning": f"Judge error: {e}",
        }


# ---------------------------------------------------------------------------
# Latency Measurement
# ---------------------------------------------------------------------------

def measure_latency(
    fn: Callable,
    *args: Any,
    runs: int = 3,
    **kwargs: Any,
) -> Dict[str, Any]:
    """
    Call *fn* multiple times and report latency statistics.

    The first call's result is returned as ``result``; subsequent calls
    are for timing only.

    Returns
    -------
    dict
        ``result``     – the return value of the first call.
        ``median_ms``  – median latency across all runs.
        ``min_ms``     – fastest run.
        ``max_ms``     – slowest run.
        ``all_ms``     – list of all individual run times.
    """
    timings: List[float] = []
    result = None

    for i in range(runs):
        t0 = time.perf_counter()
        r = fn(*args, **kwargs)
        elapsed_ms = (time.perf_counter() - t0) * 1000.0
        timings.append(round(elapsed_ms, 1))

        if i == 0:
            result = r

    return {
        "result": result,
        "median_ms": round(statistics.median(timings), 1),
        "min_ms": round(min(timings), 1),
        "max_ms": round(max(timings), 1),
        "all_ms": timings,
    }
