{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 ‚Äì Memory & LCEL Basics\n",
        "\n",
        "**Learning Goals:**\n",
        "- Understand conversational memory in LangChain\n",
        "- Compare memory types: Buffer vs Summary\n",
        "- Master LCEL (LangChain Expression Language) composition\n",
        "- Build streaming, retry, and fallback patterns\n",
        "\n",
        "**What we'll cover:**\n",
        "1. **Section A: Memory 101** - Buffer and Summary memory patterns\n",
        "2. **Section B: Memory in Chains** - Inject memory into conversational flows\n",
        "3. **Section C: LCEL Basics** - Compose runnables with `|` operator\n",
        "4. **Section D: Advanced LCEL** - Streaming, retry, fallbacks\n",
        "\n",
        "**Prerequisites:** Notebooks 01 & 02 completed\n",
        "\n",
        "**Note:** This notebook focuses on fundamentals, not RAG. No ChromaDB or retrieval here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Working directory: /Users/machinelearningzuu/Dropbox/Zuu Crew/Courses/üöß AI Engineer Essentials/Live Classes/Week 03\n",
            "‚úÖ Config loaded:\n",
            "  LLM: openrouter (openai/gpt-4o-mini)\n",
            "  Embeddings: sbert / sentence-transformers/all-MiniLM-L6-v2\n",
            "  Temperature: 0.2\n",
            "  Artifacts: ./artifacts\n",
            "  Note: Temperature is 0.2 (good for conversational demos)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/machinelearningzuu/Dropbox/Zuu Crew/Courses/üöß AI Engineer Essentials/Live Classes/Week 03/src/services/llm_services.py:375: UserWarning: ‚ö†Ô∏è  GROQ_API_KEY not found in environment\n",
            "  warnings.warn(f\"‚ö†Ô∏è  {key} not found in environment\")\n",
            "/Users/machinelearningzuu/Dropbox/Zuu Crew/Courses/üöß AI Engineer Essentials/Live Classes/Week 03/src/services/llm_services.py:375: UserWarning: ‚ö†Ô∏è  GOOGLE_API_KEY not found in environment\n",
            "  warnings.warn(f\"‚ö†Ô∏è  {key} not found in environment\")\n",
            "/Users/machinelearningzuu/Dropbox/Zuu Crew/Courses/üöß AI Engineer Essentials/Live Classes/Week 03/src/services/llm_services.py:375: UserWarning: ‚ö†Ô∏è  COHERE_API_KEY not found in environment\n",
            "  warnings.warn(f\"‚ö†Ô∏è  {key} not found in environment\")\n"
          ]
        }
      ],
      "source": [
        "# ‚öôÔ∏è Global Config & Services (using centralized modules)\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add parent directory to path and change to project root\n",
        "import os\n",
        "\n",
        "# Get the notebook's current directory and find project root\n",
        "notebook_dir = Path.cwd()\n",
        "if notebook_dir.name == \"notebooks\":\n",
        "    project_root = notebook_dir.parent\n",
        "else:\n",
        "    project_root = notebook_dir\n",
        "\n",
        "# Change to project root and add to path\n",
        "os.chdir(project_root)\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
        "\n",
        "from src.services.llm_services import (\n",
        "    load_config,\n",
        "    get_llm,\n",
        "    validate_api_keys,\n",
        "    print_config_summary\n",
        ")\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Load configuration from config.yaml (now we're in project root)\n",
        "config = load_config(\"src/config/config.yaml\")\n",
        "\n",
        "# Validate API keys\n",
        "validate_api_keys(config, verbose=True)\n",
        "\n",
        "# Print summary\n",
        "print_config_summary(config)\n",
        "print(f\"  Note: Temperature is {config['temperature']} (good for conversational demos)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LLM initialized: openrouter / gpt-4o-mini\n",
            "\n",
            "üîç Testing API connection...\n",
            "‚úÖ API key verified: API working!\n"
          ]
        }
      ],
      "source": [
        "# Initialize LLM using factory from llm_services\n",
        "llm = get_llm(config)\n",
        "print(f\"‚úÖ LLM initialized: {config['llm_provider']} / {config['llm_model']}\")\n",
        "\n",
        "# Verify API key with test completion\n",
        "print(\"\\nüîç Testing API connection...\")\n",
        "try:\n",
        "    test_response = llm.invoke(\"Say 'API working!' if you can read this.\")\n",
        "    test_msg = test_response.content if hasattr(test_response, 'content') else str(test_response)\n",
        "    print(f\"‚úÖ API key verified: {test_msg[:50]}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå API key test failed: {e}\")\n",
        "    print(\"‚ö†Ô∏è  Please check your .env file and API key configuration.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section A: Memory 101\n",
        "\n",
        "LangChain provides memory primitives to maintain conversational context across turns.\n",
        "\n",
        "### 1. ConversationBufferMemory\n",
        "\n",
        "Stores **full chat history** in memory. Simple but can grow large.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b5/fr4bwywx1hl34z7s66ljm40r0000gn/T/ipykernel_14368/1800596883.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  buffer_memory = ConversationBufferMemory(\n",
            "/var/folders/b5/fr4bwywx1hl34z7s66ljm40r0000gn/T/ipykernel_14368/1800596883.py:13: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
        "\n",
        "# ConversationBufferMemory: Stores FULL chat history in memory\n",
        "buffer_memory = ConversationBufferMemory(\n",
        "    return_messages=False,  # return_messages: Format of stored history\n",
        "                            #   False = string format \"Human: ... AI: ...\"\n",
        "                            #   True = list of Message objects (better for LCEL)\n",
        "    k=2                     # k: (Note: ignored in BufferMemory, only used in WindowMemory)\n",
        ")\n",
        "\n",
        "# ConversationChain: Pre-built chain that manages conversation flow\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,                # llm: Language model for generating responses\n",
        "    memory=buffer_memory,   # memory: Memory object to store conversation history\n",
        "    verbose=False           # verbose: If True, prints internal prompts (for debugging)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human:  hi\n",
            "AI: Hello! How are you today? Is there anything specific you'd like to chat about or ask? I'm here to help!\n",
            "Human:  goog morning\n",
            "AI: Good morning! I hope you're having a great start to your day. Do you have any plans for today, or is there something on your mind that you'd like to discuss?\n",
            "Human:  my name is Isuru\n",
            "AI: Nice to meet you, Isuru! That's a lovely name. Where are you from, or what do you enjoy doing in your free time? I'm curious to learn more about you!\n",
            "Human:  do you know my name ?\n",
            "AI: Yes, you just told me your name is Isuru! It's great to know you. If you'd like to share more about yourself or ask anything, feel free!\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "    response = conversation.predict(input=user_input)\n",
        "    print(\"Human: \", user_input)\n",
        "    print(f\"AI: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human:  Hello\n",
            "AI: Hello! How are you today? I'm here to chat about anything on your mind, whether it's a question, a topic you're interested in, or just some friendly banter. What would you like to talk about?\n",
            "Human:  DO you know my name ?\n",
            "AI: I don't know your name yet! But I'd love to learn it if you'd like to share. What should I call you?\n",
            "Human:  My name is ISuru ?\n",
            "AI: Nice to meet you, Isuru! That's a great name. How can I assist you today? Do you have any specific topics in mind or something you'd like to chat about?\n",
            "Human:  DO you know my name ?\n",
            "AI: I don't know your name yet! But I'd love to learn it if you'd like to share. What should I call you?\n",
            "Human:  Its Isuru\n",
            "AI: Got it, Isuru! Thanks for reminding me. What would you like to talk about today? Any specific interests or questions on your mind?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='My name is ISuru ?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Nice to meet you, Isuru! That's a great name. How can I assist you today? Do you have any specific topics in mind or something you'd like to chat about?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='DO you know my name ?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"I don't know your name yet! But I'd love to learn it if you'd like to share. What should I call you?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Its Isuru', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Got it, Isuru! Thanks for reminding me. What would you like to talk about today? Any specific interests or questions on your mind?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ConversationBufferWindowMemory: Only keeps the last 'k' conversation turns\n",
        "buffer_memory = ConversationBufferWindowMemory(\n",
        "    return_messages=True,   # return_messages: Return as Message objects (better for LCEL)\n",
        "    k=3                     # k: Number of conversation turns to keep\n",
        "                            #   k=3 means last 3 human-AI exchanges are remembered\n",
        "                            #   Older messages are dropped (sliding window)\n",
        ")\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=buffer_memory,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Interactive conversation loop (type 'exit' to quit)\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "    response = conversation.predict(input=user_input)\n",
        "    print(\"Human: \", user_input)\n",
        "    print(f\"AI: {response}\")\n",
        "\n",
        "# View stored history (only last k=3 turns will be shown)\n",
        "buffer_memory.load_memory_variables({})['history']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. ConversationSummaryMemory\n",
        "\n",
        "Instead of storing full history, **summarizes** past conversation using an LLM. Reduces token usage but may lose details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b5/fr4bwywx1hl34z7s66ljm40r0000gn/T/ipykernel_14368/1213597054.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  summary_memory = ConversationSummaryMemory(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Summary Memory:\n",
            "{'history': [SystemMessage(content='The human introduces herself as Alice, and the AI responds by greeting her and expressing pleasure in meeting her. The human then asks the AI for her name, and the AI confirms that her name is Alice. The human inquires about the capital of France, and the AI informs her that it is Paris.', additional_kwargs={}, response_metadata={})]}\n",
            "\n",
            "üìä Summary is more compact than full buffer\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "# ConversationSummaryMemory: Summarizes history using LLM (compact but loses detail)\n",
        "summary_memory = ConversationSummaryMemory(\n",
        "    llm=llm,              # llm: Required! Uses this LLM to generate summaries\n",
        "    return_messages=True  # return_messages: Return as Message objects\n",
        "                          #   The summary is stored as a SystemMessage\n",
        ")\n",
        "\n",
        "# Simulate a conversation by manually adding context\n",
        "# save_context(inputs, outputs) - saves a single turn\n",
        "summary_memory.save_context(\n",
        "    {\"input\": \"Hi, my name is Alice.\"},      # input: User's message\n",
        "    {\"output\": \"Hello Alice! Nice to meet you.\"}  # output: AI's response\n",
        ")\n",
        "summary_memory.save_context(\n",
        "    {\"input\": \"What's my name?\"},\n",
        "    {\"output\": \"Your name is Alice.\"}\n",
        ")\n",
        "summary_memory.save_context(\n",
        "    {\"input\": \"What's the capital of France?\"},\n",
        "    {\"output\": \"The capital of France is Paris.\"}\n",
        ")\n",
        "\n",
        "# View summarized history (notice how it's condensed)\n",
        "print(\"üìù Summary Memory:\")\n",
        "print(summary_memory.load_memory_variables({}))\n",
        "print(f\"\\nüìä Summary is more compact than full buffer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The human introduces herself as Alice, and the AI responds by greeting her and expressing pleasure in meeting her. The human then asks the AI for her name, and the AI confirms that her name is Alice. The human inquires about the capital of France, and the AI informs her that it is Paris.\n"
          ]
        }
      ],
      "source": [
        "print(summary_memory.load_memory_variables({})['history'][0].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trade-offs: Buffer vs Summary\n",
        "\n",
        "| Memory Type | Pros | Cons |\n",
        "|-------------|------|------|\n",
        "| **Buffer** | Full detail, no LLM calls | Grows unbounded, context limits |\n",
        "| **Summary** | Compact, scalable | LLM calls needed, possible drift |\n",
        "\n",
        "**When to use:**\n",
        "- **Buffer**: Short conversations, need exact history\n",
        "- **Summary**: Long conversations, want cost efficiency\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section B: Memory in Chains\n",
        "\n",
        "Let's inject memory into a simple conversational chain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üó®Ô∏è  Conversational Chain with Memory\n",
            "\n",
            "User: Hi, I'm Bob and I love Python programming.\n",
            "AI: Hello, Bob! It's great to meet you! Python is such a versatile and powerful programming language. What do you enjoy most about it? Are you working on any specific projects or exploring particular libraries? There‚Äôs so much you can do with Python, from web development with frameworks like Django and Flask to data analysis with libraries like Pandas and NumPy!\n",
            "\n",
            "User: What's my name?\n",
            "AI: Your name is Bob! It's nice to chat with you. Do you have any favorite Python projects or libraries you'd like to share?\n",
            "\n",
            "User: What do I love?\n",
            "AI: You love Python programming! It's a fantastic language with a wide range of applications. What aspects of Python do you find most enjoyable? Is it the simplicity of the syntax, the vast ecosystem of libraries, or perhaps the community support?\n",
            "\n",
            "üìù Stored Memory:\n",
            "{'history': \"Human: Hi, I'm Bob and I love Python programming.\\nAI: Hello, Bob! It's great to meet you! Python is such a versatile and powerful programming language. What do you enjoy most about it? Are you working on any specific projects or exploring particular libraries? There‚Äôs so much you can do with Python, from web development with frameworks like Django and Flask to data analysis with libraries like Pandas and NumPy!\\nHuman: What's my name?\\nAI: Your name is Bob! It's nice to chat with you. Do you have any favorite Python projects or libraries you'd like to share?\\nHuman: What do I love?\\nAI: You love Python programming! It's a fantastic language with a wide range of applications. What aspects of Python do you find most enjoyable? Is it the simplicity of the syntax, the vast ecosystem of libraries, or perhaps the community support?\"}\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Create a conversational chain with memory\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=False,  # Set to True to see internal prompts\n",
        ")\n",
        "\n",
        "# Multi-turn conversation\n",
        "print(\"üó®Ô∏è  Conversational Chain with Memory\\n\")\n",
        "\n",
        "response1 = conversation.predict(input=\"Hi, I'm Bob and I love Python programming.\")\n",
        "print(f\"User: Hi, I'm Bob and I love Python programming.\")\n",
        "print(f\"AI: {response1}\\n\")\n",
        "\n",
        "response2 = conversation.predict(input=\"What's my name?\")\n",
        "print(f\"User: What's my name?\")\n",
        "print(f\"AI: {response2}\\n\")\n",
        "\n",
        "response3 = conversation.predict(input=\"What do I love?\")\n",
        "print(f\"User: What do I love?\")\n",
        "print(f\"AI: {response3}\\n\")\n",
        "\n",
        "# View memory\n",
        "print(\"üìù Stored Memory:\")\n",
        "print(memory.load_memory_variables({}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resetting Memory\n",
        "\n",
        "Between sessions, clear memory to start fresh.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After clearing memory:\n",
            "User: What's my name?\n",
            "AI: I‚Äôm not sure what your name is! I don‚Äôt have access to that information. But I‚Äôd love to know it if you‚Äôd like to share!\n",
            "\n",
            "‚úÖ Memory reset - AI no longer remembers Bob\n"
          ]
        }
      ],
      "source": [
        "# Clear memory\n",
        "memory.clear()\n",
        "\n",
        "response4 = conversation.predict(input=\"What's my name?\")\n",
        "print(f\"After clearing memory:\")\n",
        "print(f\"User: What's my name?\")\n",
        "print(f\"AI: {response4}\")\n",
        "print(f\"\\n‚úÖ Memory reset - AI no longer remembers Bob\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section C: LCEL (LangChain Expression Language) Basics\n",
        "\n",
        "LCEL is a declarative way to compose LangChain components using the `|` operator.\n",
        "\n",
        "### Core Concepts\n",
        "\n",
        "1. **Runnable**: Base interface for all LCEL components\n",
        "2. **Pipe (`|`)**: Chain runnables together\n",
        "3. **RunnablePassthrough**: Pass data through unchanged\n",
        "4. **RunnableMap**: Apply multiple operations in parallel\n",
        "\n",
        "### Simple LCEL Chain\n",
        "\n",
        "Let's build: `PromptTemplate | LLM | StrOutputParser`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîó Simple LCEL Chain:\n",
            "Question: What is eczema and how is it treated?\n",
            "Answer: Eczema, also known as atopic dermatitis, is a chronic inflammatory skin condition characterized by dry, itchy, and inflamed skin. It can occur in various forms and may be triggered by allergens, irritants, stress, or changes in temperature.\n",
            "\n",
            "**Treatment options include:**\n",
            "\n",
            "1. **Moisturizers:** Regular use of emollients to keep the skin hydrated.\n",
            "2. **Topical corticosteroids:** To reduce inflammation and itching during flare-ups.\n",
            "3. **Topical calcineurin inhibitors:** Non-steroidal medications to control inflammation.\n",
            "4. **Antihistamines:** To relieve itching, especially at night.\n",
            "5. **Phototherapy:** Controlled exposure to ultraviolet light for severe cases.\n",
            "6. **Systemic medications:** In severe cases, oral or injectable medications may be prescribed.\n",
            "7. **Avoiding triggers:** Identifying and avoiding known irritants or allergens.\n",
            "\n",
            "Consulting a healthcare provider for a personalized treatment plan is recommended.\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# ChatPromptTemplate: Defines the structure of messages sent to the LLM\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Answer concisely.\"),  # System instruction\n",
        "    (\"human\", \"{question}\")  # {question} is a placeholder filled at runtime\n",
        "])\n",
        "\n",
        "# Build LCEL chain using the | (pipe) operator\n",
        "# Data flows: Input Dict ‚Üí Prompt ‚Üí LLM ‚Üí Output Parser ‚Üí String\n",
        "chain = (\n",
        "    prompt              # Step 1: Format input into messages\n",
        "    | llm               # Step 2: Send to LLM, get response\n",
        "    | StrOutputParser() # Step 3: Extract string from AIMessage\n",
        ")\n",
        "\n",
        "# Invoke the chain with input dictionary\n",
        "response = chain.invoke({\"question\": \"What is eczema and how is it treated?\"})\n",
        "print(\"üîó Simple LCEL Chain:\")\n",
        "print(f\"Question: What is eczema and how is it treated?\")\n",
        "print(f\"Answer: {response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RunnablePassthrough & RunnableMap\n",
        "\n",
        "Use `RunnablePassthrough` to pass input data and `RunnableMap` (via dict) for parallel operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîó Chain with Context:\n",
            "Result: Zuu Crew AI is delivering the Agentic AI Engineering Bootcamp.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "# RunnableParallel: Runs multiple operations in parallel and merges results\n",
        "# Useful for preparing multiple inputs for a prompt\n",
        "\n",
        "# Create a context-aware prompt template\n",
        "context_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Use the context to answer the question.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\")\n",
        "\n",
        "# Build chain with RunnableParallel for multiple inputs\n",
        "chain_with_context = (\n",
        "    RunnableParallel({\n",
        "        \"context\": RunnablePassthrough(),   # Pass context through unchanged\n",
        "        \"question\": RunnablePassthrough()   # Pass question through unchanged\n",
        "    })\n",
        "    | context_prompt    # Format into prompt with both placeholders\n",
        "    | llm               # Generate answer\n",
        "    | StrOutputParser() # Extract string\n",
        ")\n",
        "\n",
        "# Test the chain\n",
        "result = chain_with_context.invoke({\n",
        "    \"context\": \"Zuu Crew AI is delivering Agentic AI Engineering Bootcamp.\",\n",
        "    \"question\": \"What bootcamps is Zuu Crew AI doing?\"\n",
        "})\n",
        "\n",
        "print(\"üîó Chain with Context:\")\n",
        "print(f\"Result: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section D: Advanced LCEL Patterns\n",
        "\n",
        "### 1. Streaming\n",
        "\n",
        "Stream tokens as they're generated for better UX.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåä Without Streaming (waits for complete response):\n",
            "RAG, or Retrieval-Augmented Generation, is a machine learning approach that combines information retrieval with generative models to enhance the generation of text by incorporating relevant external knowledge.\n",
            "\n",
            "üåä With Streaming (tokens appear as generated):\n",
            "Answer: RAG (Retrieval-Augmented Generation) is a machine learning approach that combines retrieval of relevant documents from a knowledge base with generative models to produce more accurate and contextually relevant responses.\n",
            "\n",
            "‚úÖ Streaming complete\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# Compare: Regular invoke (waits for full response) vs Streaming (token by token)\n",
        "\n",
        "print(\"üåä Without Streaming (waits for complete response):\")\n",
        "print(chain.invoke({\"question\": \"Explain RAG in one sentence.\"}))\n",
        "\n",
        "print(\"\\nüåä With Streaming (tokens appear as generated):\")\n",
        "print(\"Answer: \", end=\"\")\n",
        "\n",
        "# chain.stream() yields chunks as they're generated\n",
        "for chunk in chain.stream({\"question\": \"Explain RAG in one sentence.\"}):\n",
        "    print(chunk, end=\"\", flush=True)  # end=\"\" prevents newlines, flush=True forces immediate output\n",
        "\n",
        "print(\"\\n\\n‚úÖ Streaming complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Retry with Fallback\n",
        "\n",
        "Use `.with_retry()` for automatic retries and `.with_fallbacks()` for fallback models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Chain with retry enabled\n",
            "   - Retries up to 3 times on API failures\n",
            "‚úÖ Retry pattern configured\n"
          ]
        }
      ],
      "source": [
        "# .with_retry(): Automatically retry on transient failures\n",
        "chain_with_retry = chain.with_retry(\n",
        "    stop_after_attempt=3  # stop_after_attempt: Max number of retry attempts\n",
        "                          #   3 = try up to 3 times before raising error\n",
        "                          # Other options: wait_exponential_jitter=True for backoff\n",
        ")\n",
        "\n",
        "print(\"üîÑ Chain with retry enabled\")\n",
        "print(\"   - Retries up to 3 times on API failures\")\n",
        "\n",
        "# .with_fallbacks(): Use backup LLM if primary fails\n",
        "# Example (requires a second LLM configured):\n",
        "# fallback_llm = get_llm({\"llm_provider\": \"groq\", ...})\n",
        "# chain_with_fallback = chain.with_fallbacks([fallback_llm | StrOutputParser()])\n",
        "# print(\"   - Falls back to secondary LLM if primary fails\")\n",
        "\n",
        "print(\"‚úÖ Retry pattern configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Save Manifest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Manifest saved: artifacts/manifests/memory_lcel.json\n"
          ]
        }
      ],
      "source": [
        "manifests_dir = Path(config[\"artifacts_root\"]) / \"manifests\"\n",
        "manifests_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "manifest = {\n",
        "    \"notebook\": \"03_memory_lcel_basics\",\n",
        "    \"topics\": [\n",
        "        \"ConversationBufferMemory\",\n",
        "        \"ConversationBufferWindowMemory\", \n",
        "        \"ConversationSummaryMemory\",\n",
        "        \"ConversationChain with memory\",\n",
        "        \"LCEL composition (pipe operator)\",\n",
        "        \"RunnableParallel\",\n",
        "        \"Streaming\",\n",
        "        \"Retry patterns\"\n",
        "    ],\n",
        "    \"llm_provider\": config[\"llm_provider\"],\n",
        "    \"llm_model\": config[\"llm_model\"],\n",
        "    \"created_at\": datetime.now().isoformat(),\n",
        "}\n",
        "\n",
        "manifest_path = manifests_dir / \"memory_lcel.json\"\n",
        "with open(manifest_path, \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Manifest saved: {manifest_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "**What we learned:**\n",
        "\n",
        "### Memory\n",
        "- ‚úÖ **Buffer Memory**: Stores full history (simple but grows)\n",
        "- ‚úÖ **Summary Memory**: LLM-summarized history (compact but may drift)\n",
        "- ‚úÖ **Memory in Chains**: Inject context into conversational flows\n",
        "- ‚úÖ **Reset/Clear**: Start fresh between sessions\n",
        "\n",
        "### LCEL\n",
        "- ‚úÖ **Composition**: Use `|` to chain runnables\n",
        "- ‚úÖ **RunnablePassthrough**: Pass data unchanged\n",
        "- ‚úÖ **RunnableParallel**: Run operations in parallel\n",
        "- ‚úÖ **Streaming**: Token-by-token generation\n",
        "- ‚úÖ **Retry**: Automatic retries on failure\n",
        "- ‚úÖ **Fallbacks**: Switch to backup LLM\n",
        "\n",
        "**Key Patterns:**\n",
        "```python\n",
        "# Simple chain\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "# With context\n",
        "chain = RunnableParallel({...}) | prompt | llm | parser\n",
        "\n",
        "# With retry\n",
        "chain = chain.with_retry(stop_after_attempt=3)\n",
        "\n",
        "# With streaming\n",
        "for chunk in chain.stream(input):\n",
        "    print(chunk)\n",
        "```\n",
        "\n",
        "**Artifacts:**\n",
        "- `./artifacts/manifests/memory_lcel.json`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
