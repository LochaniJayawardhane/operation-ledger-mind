{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29eeeb3c",
   "metadata": {},
   "source": [
    "## Setup: imports and environment checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f1c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, gc\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Add project root to path (same pattern as notebooks/01_data_factory.ipynb)\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "\n",
    "def print_vram():\n",
    "    \"\"\"Print current GPU VRAM usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        alloc = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
    "        print(f\"  VRAM: {alloc:.1f}GB allocated / {reserved:.1f}GB reserved / {total:.1f}GB total\")\n",
    "\n",
    "def clear_vram():\n",
    "    \"\"\"Free unused VRAM.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"✓ Project root: {project_root}\")\n",
    "print(f\"✓ torch: {torch.__version__}\")\n",
    "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print_vram()\n",
    "\n",
    "# QLoRA needs bitsandbytes (usually Linux/Colab)\n",
    "try:\n",
    "    import bitsandbytes as bnb  # noqa: F401\n",
    "    print(\"✓ bitsandbytes import: OK\")\n",
    "except Exception as e:\n",
    "    print(\"✗ bitsandbytes import failed. If you are on Windows, run this notebook in Colab or WSL.\")\n",
    "    print(\"  Error:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a486ca9f",
   "metadata": {},
   "source": [
    "## Hugging Face Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da968a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Login via Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get(\"HF_TOKEN\")\n",
    "    login(token=hf_token)\n",
    "    print(\"✓ Logged in to Hugging Face via Colab secret (HF_TOKEN)\")\n",
    "except Exception:\n",
    "    # Option 2: Login via environment variable\n",
    "    import os\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\", \"\")\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"✓ Logged in to Hugging Face via HF_TOKEN env variable\")\n",
    "    else:\n",
    "        # Option 3: Interactive login (will prompt for token)\n",
    "        print(\"⚠ No HF_TOKEN found. Running interactive login...\")\n",
    "        print(\"  Get your token at: https://huggingface.co/settings/tokens\")\n",
    "        login()\n",
    "        print(\"✓ Logged in to Hugging Face interactively\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f3384c",
   "metadata": {},
   "source": [
    "## Load config + paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b4bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.config_loader import load_config\n",
    "from src.finetuning.data import resolve_finetune_paths\n",
    "\n",
    "config_path = project_root / \"config\" / \"config.yaml\"\n",
    "config = load_config(config_path)\n",
    "paths = resolve_finetune_paths(config)\n",
    "\n",
    "print(\"✓ Base model:\", config[\"finetuning\"][\"base_model\"])\n",
    "print(\"✓ Train file:\", paths.train_file)\n",
    "print(\"✓ Eval file:\", paths.eval_file)\n",
    "print(\"✓ Output dir:\", paths.output_dir)\n",
    "\n",
    "print(\"✓ Files exist:\")\n",
    "print(\"  - train:\", paths.train_file.exists())\n",
    "print(\"  - eval:\", paths.eval_file.exists() if paths.eval_file else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd6496",
   "metadata": {},
   "source": [
    "## Setup: BitsAndBytesConfig + LoraConfig (QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac513709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "from src.finetuning.data import load_sft_datasets\n",
    "\n",
    "ft_cfg = config[\"finetuning\"]\n",
    "tr_cfg = ft_cfg[\"training\"]\n",
    "\n",
    "base_model = ft_cfg[\"base_model\"]\n",
    "compute_dtype_str = ft_cfg.get(\"quantization\", {}).get(\"compute_dtype\", \"float16\")\n",
    "compute_dtype = torch.float16 if str(compute_dtype_str).lower() in {\"float16\", \"fp16\"} else torch.bfloat16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=ft_cfg.get(\"quantization\", {}).get(\"quant_type\", \"nf4\"),\n",
    "    bnb_4bit_use_double_quant=bool(ft_cfg.get(\"quantization\", {}).get(\"double_quant\", True)),\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=int(ft_cfg.get(\"lora\", {}).get(\"r\", 16)),\n",
    "    lora_alpha=int(ft_cfg.get(\"lora\", {}).get(\"alpha\", 32)),\n",
    "    lora_dropout=float(ft_cfg.get(\"lora\", {}).get(\"dropout\", 0.05)),\n",
    "    target_modules=list(ft_cfg.get(\"lora\", {}).get(\"target_modules\", [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"])),\n",
    "    bias=str(ft_cfg.get(\"lora\", {}).get(\"bias\", \"none\")),\n",
    "    task_type=str(ft_cfg.get(\"lora\", {}).get(\"task_type\", \"CAUSAL_LM\")),\n",
    ")\n",
    "\n",
    "print(\"✓ BitsAndBytesConfig:\", bnb_config)\n",
    "print(\"✓ LoraConfig target_modules:\", lora_config.target_modules)\n",
    "\n",
    "# Tokenizer\n",
    "trust_remote_code = bool(config.get(\"providers\", {}).get(\"huggingface\", {}).get(\"trust_remote_code\", True))\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True, trust_remote_code=trust_remote_code)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Dataset → SFT-ready `text`\n",
    "max_seq_length = int(tr_cfg.get(\"max_seq_length\", 2048))\n",
    "ds = load_sft_datasets(\n",
    "    train_file=paths.train_file,\n",
    "    eval_file=paths.eval_file,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "# Free any stale VRAM before loading the model\n",
    "clear_vram()\n",
    "\n",
    "# Model (4-bit) + LoRA\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=config.get(\"providers\", {}).get(\"huggingface\", {}).get(\"device_map\", \"auto\"),\n",
    "    trust_remote_code=trust_remote_code,\n",
    "    torch_dtype=compute_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable, total = 0, 0\n",
    "for p in model.parameters():\n",
    "    total += p.numel()\n",
    "    if p.requires_grad:\n",
    "        trainable += p.numel()\n",
    "\n",
    "print(\"✓ Model prepared for QLoRA\")\n",
    "print(f\"  Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n",
    "print_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad0a4ab",
   "metadata": {},
   "source": [
    "## Training: SFTTrainer loop (≥ 100 steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4113976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For speed while testing, you can uncomment the next two lines.\n",
    "# ds[\"train\"] = ds[\"train\"].select(range(min(256, len(ds[\"train\"]))))\n",
    "# if \"eval\" in ds: ds[\"eval\"] = ds[\"eval\"].select(range(min(64, len(ds[\"eval\"]))))\n",
    "\n",
    "out_dir = Path(paths.output_dir)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "min_steps = int(tr_cfg.get(\"min_steps\", 100))\n",
    "max_steps = max(min_steps, int(tr_cfg.get(\"max_steps\", min_steps)))\n",
    "\n",
    "sft_args = SFTConfig(\n",
    "    output_dir=str(out_dir),\n",
    "    max_steps=max_steps,\n",
    "    num_train_epochs=float(tr_cfg.get(\"num_epochs\", 1)),\n",
    "    per_device_train_batch_size=int(tr_cfg.get(\"batch_size\", 1)),\n",
    "    gradient_accumulation_steps=int(tr_cfg.get(\"gradient_accumulation_steps\", 1)),\n",
    "    learning_rate=float(tr_cfg.get(\"learning_rate\", 2e-4)),\n",
    "    warmup_ratio=float(tr_cfg.get(\"warmup_ratio\", 0.03)),\n",
    "    logging_steps=int(tr_cfg.get(\"logging_steps\", 10)),\n",
    "    save_steps=int(tr_cfg.get(\"save_steps\", 50)),\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=False,\n",
    "    fp16=compute_dtype == torch.float16,\n",
    "    bf16=compute_dtype == torch.bfloat16,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=str(tr_cfg.get(\"optim\", \"paged_adamw_8bit\")),  # 8-bit optimizer saves ~1-2GB VRAM\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds.get(\"eval\"),\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "print(f\"✓ SFTTrainer ready — batch_size={sft_args.per_device_train_batch_size}, \"\n",
    "      f\"grad_accum={sft_args.gradient_accumulation_steps}, \"\n",
    "      f\"max_seq_length={sft_args.max_seq_length}, \"\n",
    "      f\"optim={sft_args.optim}\")\n",
    "print_vram()\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(\"✓ Train result:\", train_result)\n",
    "print_vram()\n",
    "\n",
    "# Save adapters + tokenizer\n",
    "trainer.model.save_pretrained(str(out_dir))\n",
    "tokenizer.save_pretrained(str(out_dir))\n",
    "\n",
    "adapter_dir = out_dir\n",
    "print(\"✓ Saved adapters to:\", adapter_dir)\n",
    "print(\"✓ Adapter files:\")\n",
    "for p in sorted(Path(adapter_dir).glob(\"*\")):\n",
    "    print(\" -\", p.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ef279",
   "metadata": {},
   "source": [
    "## Training: loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_history = getattr(trainer.state, \"log_history\", [])\n",
    "steps = [x[\"step\"] for x in log_history if \"loss\" in x and \"step\" in x]\n",
    "losses = [x[\"loss\"] for x in log_history if \"loss\" in x and \"step\" in x]\n",
    "\n",
    "if not losses:\n",
    "    print(\"No loss entries found in trainer.state.log_history (try lowering logging_steps).\")\n",
    "else:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(steps, losses)\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2870b323",
   "metadata": {},
   "source": [
    "## Inference pipeline: query_intern(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3abf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Free the training model from VRAM first, then load the saved adapters\n",
    "# through the standalone query_intern() inference pipeline.\n",
    "\n",
    "# --- Free training model to reclaim VRAM ---\n",
    "import gc\n",
    "del trainer, model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"✓ Training model freed from VRAM\")\n",
    "print_vram()\n",
    "\n",
    "# --- Load inference pipeline ---\n",
    "from src.finetuning.intern_inference import query_intern\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# Demo 1: Simple call — query_intern(question)\n",
    "# This is the primary API. It loads base model + saved LoRA\n",
    "# adapters automatically from config defaults.\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"Demo 1: query_intern(question)  [no context]\")\n",
    "print(\"=\" * 60)\n",
    "answer = query_intern(\"What is Uber's Adjusted EBITDA for 2024?\")\n",
    "print(f\"\\nQuestion: What is Uber's Adjusted EBITDA for 2024?\")\n",
    "print(f\"Answer:   {answer}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# Demo 2: With context — query_intern(question, chunk_text=...)\n",
    "# Pass a chunk as grounding context (used during RAG / eval).\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Demo 2: query_intern(question, chunk_text=...)  [with context]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "example = None\n",
    "if paths.eval_file and paths.eval_file.exists():\n",
    "    with open(paths.eval_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        example = json.loads(next(iter(f)))\n",
    "\n",
    "if example:\n",
    "    q = example[\"question\"]\n",
    "    ctx = example.get(\"chunk_text\")\n",
    "    gt = example.get(\"answer\")\n",
    "\n",
    "    pred = query_intern(q, chunk_text=ctx, config_path=config_path)\n",
    "\n",
    "    print(f\"\\nQuestion:     {q}\")\n",
    "    print(f\"Ground truth: {gt}\")\n",
    "    print(f\"Intern:       {pred}\")\n",
    "else:\n",
    "    pred = query_intern(\"What is Adjusted EBITDA?\", config_path=config_path)\n",
    "    print(f\"\\nIntern: {pred}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# Demo 3: \"Information not available\" — question outside context\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Demo 3: Out-of-scope question (should say info not available)\")\n",
    "print(\"=\" * 60)\n",
    "oos = query_intern(\n",
    "    \"What was Apple's revenue in 2024?\",\n",
    "    chunk_text=\"Uber's total revenue for 2024 was $43.9 billion.\",\n",
    "    config_path=config_path,\n",
    ")\n",
    "print(f\"\\nQuestion: What was Apple's revenue in 2024?\")\n",
    "print(f\"Context:  Uber's total revenue for 2024 was $43.9 billion.\")\n",
    "print(f\"Intern:   {oos}\")\n",
    "\n",
    "print(\"\\n✓ Inference pipeline verified\")\n",
    "print_vram()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cec975",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Optional) Format preview: inspect a single SFT training text\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from src.finetuning.data import format_sft_record\n",
    "\n",
    "base_model = config[\"finetuning\"][\"base_model\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "\n",
    "with open(paths.train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    rec = json.loads(next(iter(f)))\n",
    "\n",
    "formatted = format_sft_record(rec, tokenizer, max_seq_length=config[\"finetuning\"][\"training\"][\"max_seq_length\"])\n",
    "print(formatted[\"text\"][:1200])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
