{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95417de",
   "metadata": {},
   "source": [
    "# Fine-tune “The Intern” (QLoRA + TRL)\n",
    "\n",
    "This notebook is the **main fine-tuning pipeline** for Operation Ledger-Mind.\n",
    "\n",
    "It will:\n",
    "\n",
    "- Load `config/config.yaml`\n",
    "- Load your JSONL dataset (`data/output/train.jsonl`)\n",
    "- Format each example using the **Llama-3 chat template** (system/user/assistant)\n",
    "- Fine-tune with **QLoRA** (4-bit NF4 + LoRA) using **TRL SFTTrainer** for **≥ 100 steps**\n",
    "- Save adapters to `finetuning.output_dir` (default: `models/intern_adapter`)\n",
    "- Run a quick inference smoke test via `query_intern(...)`\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Llama-3 weights are gated: you may need `huggingface-cli login`.\n",
    "- QLoRA typically requires a CUDA GPU + `bitsandbytes` (Colab T4 works well).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f1c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup: imports and environment checks\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Add project root to path (same pattern as notebooks/01_data_factory.ipynb)\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f\"✓ Project root: {project_root}\")\n",
    "print(f\"✓ torch: {torch.__version__}\")\n",
    "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# QLoRA needs bitsandbytes (usually Linux/Colab)\n",
    "try:\n",
    "    import bitsandbytes as bnb  # noqa: F401\n",
    "    print(\"✓ bitsandbytes import: OK\")\n",
    "except Exception as e:\n",
    "    print(\"✗ bitsandbytes import failed. If you are on Windows, run this notebook in Colab or WSL.\")\n",
    "    print(\"  Error:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b4bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load config + paths\n",
    "\n",
    "from src.utils.config_loader import load_config\n",
    "from src.finetuning.data import resolve_finetune_paths\n",
    "\n",
    "config_path = project_root / \"config\" / \"config.yaml\"\n",
    "config = load_config(config_path)\n",
    "paths = resolve_finetune_paths(config)\n",
    "\n",
    "print(\"✓ Base model:\", config[\"finetuning\"][\"base_model\"])\n",
    "print(\"✓ Train file:\", paths.train_file)\n",
    "print(\"✓ Eval file:\", paths.eval_file)\n",
    "print(\"✓ Output dir:\", paths.output_dir)\n",
    "\n",
    "print(\"✓ Files exist:\")\n",
    "print(\"  - train:\", paths.train_file.exists())\n",
    "print(\"  - eval:\", paths.eval_file.exists() if paths.eval_file else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac513709",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup: BitsAndBytesConfig + LoraConfig (QLoRA)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "from src.finetuning.data import load_sft_datasets\n",
    "\n",
    "ft_cfg = config[\"finetuning\"]\n",
    "tr_cfg = ft_cfg[\"training\"]\n",
    "\n",
    "base_model = ft_cfg[\"base_model\"]\n",
    "compute_dtype_str = ft_cfg.get(\"quantization\", {}).get(\"compute_dtype\", \"float16\")\n",
    "compute_dtype = torch.float16 if str(compute_dtype_str).lower() in {\"float16\", \"fp16\"} else torch.bfloat16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=ft_cfg.get(\"quantization\", {}).get(\"quant_type\", \"nf4\"),\n",
    "    bnb_4bit_use_double_quant=bool(ft_cfg.get(\"quantization\", {}).get(\"double_quant\", True)),\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=int(ft_cfg.get(\"lora\", {}).get(\"r\", 16)),\n",
    "    lora_alpha=int(ft_cfg.get(\"lora\", {}).get(\"alpha\", 32)),\n",
    "    lora_dropout=float(ft_cfg.get(\"lora\", {}).get(\"dropout\", 0.05)),\n",
    "    target_modules=list(ft_cfg.get(\"lora\", {}).get(\"target_modules\", [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"])),\n",
    "    bias=str(ft_cfg.get(\"lora\", {}).get(\"bias\", \"none\")),\n",
    "    task_type=str(ft_cfg.get(\"lora\", {}).get(\"task_type\", \"CAUSAL_LM\")),\n",
    ")\n",
    "\n",
    "print(\"✓ BitsAndBytesConfig:\", bnb_config)\n",
    "print(\"✓ LoraConfig target_modules:\", lora_config.target_modules)\n",
    "\n",
    "# Tokenizer\n",
    "trust_remote_code = bool(config.get(\"providers\", {}).get(\"huggingface\", {}).get(\"trust_remote_code\", True))\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True, trust_remote_code=trust_remote_code)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Dataset → SFT-ready `text`\n",
    "max_seq_length = int(tr_cfg.get(\"max_seq_length\", 2048))\n",
    "ds = load_sft_datasets(\n",
    "    train_file=paths.train_file,\n",
    "    eval_file=paths.eval_file,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "# Model (4-bit) + LoRA\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=config.get(\"providers\", {}).get(\"huggingface\", {}).get(\"device_map\", \"auto\"),\n",
    "    trust_remote_code=trust_remote_code,\n",
    "    torch_dtype=compute_dtype,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"✓ Model prepared for QLoRA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4113976",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training: SFTTrainer loop (≥ 100 steps)\n",
    "\n",
    "# For speed while testing, you can uncomment the next two lines.\n",
    "# ds[\"train\"] = ds[\"train\"].select(range(min(256, len(ds[\"train\"]))))\n",
    "# if \"eval\" in ds: ds[\"eval\"] = ds[\"eval\"].select(range(min(64, len(ds[\"eval\"]))))\n",
    "\n",
    "out_dir = Path(paths.output_dir)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "min_steps = int(tr_cfg.get(\"min_steps\", 100))\n",
    "max_steps = max(min_steps, int(tr_cfg.get(\"max_steps\", min_steps)))\n",
    "\n",
    "sft_args = SFTConfig(\n",
    "    output_dir=str(out_dir),\n",
    "    max_steps=max_steps,\n",
    "    num_train_epochs=float(tr_cfg.get(\"num_epochs\", 1)),\n",
    "    per_device_train_batch_size=int(tr_cfg.get(\"batch_size\", 1)),\n",
    "    gradient_accumulation_steps=int(tr_cfg.get(\"gradient_accumulation_steps\", 1)),\n",
    "    learning_rate=float(tr_cfg.get(\"learning_rate\", 2e-4)),\n",
    "    warmup_ratio=float(tr_cfg.get(\"warmup_ratio\", 0.03)),\n",
    "    logging_steps=int(tr_cfg.get(\"logging_steps\", 10)),\n",
    "    save_steps=int(tr_cfg.get(\"save_steps\", 50)),\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=False,\n",
    "    fp16=compute_dtype == torch.float16,\n",
    "    bf16=compute_dtype == torch.bfloat16,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds.get(\"eval\"),\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(\"✓ Train result:\", train_result)\n",
    "\n",
    "# Save adapters + tokenizer\n",
    "trainer.model.save_pretrained(str(out_dir))\n",
    "tokenizer.save_pretrained(str(out_dir))\n",
    "\n",
    "adapter_dir = out_dir\n",
    "print(\"✓ Saved adapters to:\", adapter_dir)\n",
    "print(\"✓ Adapter files:\")\n",
    "for p in sorted(Path(adapter_dir).glob(\"*\")):\n",
    "    print(\" -\", p.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training: loss curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_history = getattr(trainer.state, \"log_history\", [])\n",
    "steps = [x[\"step\"] for x in log_history if \"loss\" in x and \"step\" in x]\n",
    "losses = [x[\"loss\"] for x in log_history if \"loss\" in x and \"step\" in x]\n",
    "\n",
    "if not losses:\n",
    "    print(\"No loss entries found in trainer.state.log_history (try lowering logging_steps).\")\n",
    "else:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(steps, losses)\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3abf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Smoke-test inference with query_intern()\n",
    "\n",
    "from src.finetuning.intern_inference import query_intern\n",
    "\n",
    "# Use an example from the golden test set for a realistic (question + context) call\n",
    "example = None\n",
    "if paths.eval_file and paths.eval_file.exists():\n",
    "    with open(paths.eval_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        example = json.loads(next(iter(f)))\n",
    "\n",
    "if example:\n",
    "    q = example[\"question\"]\n",
    "    ctx = example.get(\"chunk_text\")\n",
    "    gt = example.get(\"answer\")\n",
    "\n",
    "    print(\"Question:\", q)\n",
    "    print(\"\\nGround truth (dataset):\", gt)\n",
    "\n",
    "    pred = query_intern(q, chunk_text=ctx, config_path=config_path)\n",
    "    print(\"\\nIntern output:\", pred)\n",
    "else:\n",
    "    # Fallback: no context (less faithful, but checks the codepath)\n",
    "    print(query_intern(\"What is Adjusted EBITDA?\", config_path=config_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cec975",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Optional) Format preview: inspect a single SFT training text\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from src.finetuning.data import format_sft_record\n",
    "\n",
    "base_model = config[\"finetuning\"][\"base_model\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "\n",
    "with open(paths.train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    rec = json.loads(next(iter(f)))\n",
    "\n",
    "formatted = format_sft_record(rec, tokenizer, max_seq_length=config[\"finetuning\"][\"training\"][\"max_seq_length\"])\n",
    "print(formatted[\"text\"][:1200])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
