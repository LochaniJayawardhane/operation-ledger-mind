{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab-only: install required packages\n",
        "# (skip this cell if running locally)\n",
        "%pip install -q \"rouge-score>=0.1.2\" \"weaviate-client>=4.4.0\" \"sentence-transformers>=2.7.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: The Showdown ‚Äî Evaluation Arena\n",
        "\n",
        "Compare **The Intern** (fine-tuned Llama-3) vs **The Librarian** (Hybrid RAG + gpt-4o-mini)\n",
        "across three dimensions:\n",
        "\n",
        "| Metric | What it measures |\n",
        "|--------|------------------|\n",
        "| **ROUGE-L** | Textual overlap between generated and ground-truth answers |\n",
        "| **LLM-as-a-Judge** | Faithfulness & Accuracy scored 1-5 by `o3-mini` |\n",
        "| **Latency** | Response time in milliseconds |\n",
        "\n",
        "**Bonus:** Monthly cost projection for 500 daily users √ó 10 queries each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Imports and Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "matplotlib.rcParams['figure.dpi'] = 120\n",
        "\n",
        "# Add project root to path (same pattern as other notebooks)\n",
        "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
        "sys.path.insert(0, str(project_root))\n",
        "os.chdir(project_root)\n",
        "\n",
        "print(f\"‚úì Project root: {project_root}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load OpenAI API key (Colab secrets or .env)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"‚úì OpenAI API key loaded from Colab secrets\")\n",
        "except Exception:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv(project_root / '.env')\n",
        "    print(\"‚úì Environment loaded from .env\")\n",
        "\n",
        "print(f\"‚úì OPENAI_API_KEY set: {'Yes' if os.environ.get('OPENAI_API_KEY') else 'No'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils.config_loader import load_config\n",
        "from src.evaluation.metrics import compute_rouge_l, llm_judge_score, measure_latency\n",
        "from src.evaluation.cost_analysis import estimate_monthly_cost\n",
        "\n",
        "# Part 3 ‚Äì Librarian\n",
        "from src.rag.librarian_inference import query_librarian\n",
        "\n",
        "print(\"‚úì All imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config_path = project_root / 'config' / 'config.yaml'\n",
        "config = load_config(config_path)\n",
        "\n",
        "eval_cfg = config.get('evaluation', {})\n",
        "judge_model = eval_cfg.get('metrics', {}).get('llm_judge', {}).get('model', 'o3-mini')\n",
        "latency_runs = eval_cfg.get('latency', {}).get('runs_per_query', 3)\n",
        "\n",
        "print(\"‚úì Configuration loaded\")\n",
        "print(f\"  LLM Judge model : {judge_model}\")\n",
        "print(f\"  Latency runs    : {latency_runs}\")\n",
        "print(f\"  ROUGE-L enabled : {eval_cfg.get('metrics', {}).get('rouge_l', {}).get('enabled', True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load & Sample the Golden Test Set\n",
        "\n",
        "We stratified-sample **20 questions** across the three categories\n",
        "(`hard_facts`, `strategic_summary`, `stylistic_creative`) so that\n",
        "each category is proportionally represented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load full golden test set\n",
        "golden_path = project_root / 'data' / 'output' / 'golden_test_set.jsonl'\n",
        "assert golden_path.exists(), f\"Golden test set not found: {golden_path}\"\n",
        "\n",
        "with open(golden_path, 'r', encoding='utf-8') as f:\n",
        "    golden_data = [json.loads(line) for line in f if line.strip()]\n",
        "\n",
        "print(f\"‚úì Loaded {len(golden_data)} golden test entries\")\n",
        "\n",
        "# Show category distribution\n",
        "cat_counts = defaultdict(int)\n",
        "for entry in golden_data:\n",
        "    cat_counts[entry.get('category', 'unknown')] += 1\n",
        "\n",
        "print(\"\\nCategory distribution:\")\n",
        "for cat, count in sorted(cat_counts.items()):\n",
        "    print(f\"  {cat}: {count} ({count/len(golden_data)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stratified sample: 20 questions, proportional to category sizes\n",
        "SAMPLE_SIZE = 20\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "\n",
        "# Group by category\n",
        "by_category = defaultdict(list)\n",
        "for entry in golden_data:\n",
        "    by_category[entry.get('category', 'unknown')].append(entry)\n",
        "\n",
        "# Calculate proportional sample per category\n",
        "sample = []\n",
        "total = len(golden_data)\n",
        "remaining = SAMPLE_SIZE\n",
        "\n",
        "sorted_cats = sorted(by_category.keys())\n",
        "for i, cat in enumerate(sorted_cats):\n",
        "    if i == len(sorted_cats) - 1:\n",
        "        n = remaining  # last category gets whatever is left\n",
        "    else:\n",
        "        n = max(1, round(SAMPLE_SIZE * len(by_category[cat]) / total))\n",
        "    n = min(n, remaining, len(by_category[cat]))\n",
        "    sample.extend(random.sample(by_category[cat], n))\n",
        "    remaining -= n\n",
        "\n",
        "random.shuffle(sample)\n",
        "\n",
        "print(f\"‚úì Sampled {len(sample)} questions (seed={SEED})\")\n",
        "print(\"\\nSample category breakdown:\")\n",
        "sample_cats = defaultdict(int)\n",
        "for s in sample:\n",
        "    sample_cats[s['category']] += 1\n",
        "for cat, count in sorted(sample_cats.items()):\n",
        "    print(f\"  {cat}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview the sample\n",
        "sample_df = pd.DataFrame([\n",
        "    {\n",
        "        'idx': i + 1,\n",
        "        'category': s['category'],\n",
        "        'question': s['question'][:80] + ('...' if len(s['question']) > 80 else ''),\n",
        "        'answer_preview': s['answer'][:60] + ('...' if len(s['answer']) > 60 else ''),\n",
        "    }\n",
        "    for i, s in enumerate(sample)\n",
        "])\n",
        "sample_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Run The Intern (Fine-Tuned Model)\n",
        "\n",
        "The Intern uses `query_intern(question, chunk_text=...)` with the ground-truth\n",
        "chunk as context ‚Äî matching its fine-tuning setup.\n",
        "\n",
        "> **Note:** Requires a GPU with the fine-tuned adapter files. If unavailable,\n",
        "> this step will be gracefully skipped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if Intern is available\n",
        "import torch\n",
        "\n",
        "adapter_dir = project_root / config.get('finetuning', {}).get('output_dir', 'models/intern_adapter')\n",
        "has_gpu = torch.cuda.is_available()\n",
        "has_adapter = adapter_dir.exists() and any(adapter_dir.iterdir()) if adapter_dir.exists() else False\n",
        "intern_available = has_gpu and has_adapter\n",
        "\n",
        "print(f\"GPU available     : {has_gpu}\")\n",
        "print(f\"Adapter directory : {adapter_dir}\")\n",
        "print(f\"Adapter files     : {has_adapter}\")\n",
        "print(f\"\\n{'‚úì Intern is AVAILABLE ‚Äì will run evaluation' if intern_available else '‚ö†Ô∏è  Intern UNAVAILABLE ‚Äì will be skipped (need GPU + adapter files)'}\")\n",
        "\n",
        "if intern_available:\n",
        "    from src.finetuning.intern_inference import query_intern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "intern_results = []\n",
        "\n",
        "if intern_available:\n",
        "    print(f\"üîÑ Running Intern on {len(sample)} questions (latency_runs={latency_runs})...\\n\")\n",
        "    for i, entry in enumerate(sample):\n",
        "        print(f\"  [{i+1}/{len(sample)}] {entry['question'][:70]}...\", end=\" \")\n",
        "        try:\n",
        "            lat = measure_latency(\n",
        "                query_intern,\n",
        "                entry['question'],\n",
        "                chunk_text=entry.get('chunk_text', ''),\n",
        "                config_path=str(config_path),\n",
        "                runs=latency_runs,\n",
        "            )\n",
        "            intern_results.append({\n",
        "                'question': entry['question'],\n",
        "                'ground_truth': entry['answer'],\n",
        "                'prediction': lat['result'],\n",
        "                'latency_median_ms': lat['median_ms'],\n",
        "                'latency_min_ms': lat['min_ms'],\n",
        "                'latency_max_ms': lat['max_ms'],\n",
        "                'error': None,\n",
        "            })\n",
        "            print(f\"‚úì {lat['median_ms']:.0f}ms\")\n",
        "        except Exception as e:\n",
        "            intern_results.append({\n",
        "                'question': entry['question'],\n",
        "                'ground_truth': entry['answer'],\n",
        "                'prediction': f'[ERROR] {e}',\n",
        "                'latency_median_ms': None,\n",
        "                'latency_min_ms': None,\n",
        "                'latency_max_ms': None,\n",
        "                'error': str(e),\n",
        "            })\n",
        "            print(f\"‚úó {e}\")\n",
        "    print(f\"\\n‚úì Intern complete: {sum(1 for r in intern_results if r['error'] is None)}/{len(sample)} succeeded\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Skipping Intern evaluation (GPU or adapter files not available)\")\n",
        "    for entry in sample:\n",
        "        intern_results.append({\n",
        "            'question': entry['question'],\n",
        "            'ground_truth': entry['answer'],\n",
        "            'prediction': '[SKIPPED] Intern not available',\n",
        "            'latency_median_ms': None,\n",
        "            'latency_min_ms': None,\n",
        "            'latency_max_ms': None,\n",
        "            'error': 'Intern not available (no GPU or adapter)',\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Run The Librarian (RAG Pipeline)\n",
        "\n",
        "The Librarian uses `query_librarian(question)` ‚Äî it retrieves its own context\n",
        "via hybrid search (Dense + BM25 ‚Üí RRF ‚Üí Cross-Encoder reranking) and then\n",
        "generates an answer with `gpt-4o-mini`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "librarian_results = []\n",
        "\n",
        "print(f\"üîÑ Running Librarian on {len(sample)} questions...\\n\")\n",
        "\n",
        "for i, entry in enumerate(sample):\n",
        "    print(f\"  [{i+1}/{len(sample)}] {entry['question'][:70]}...\", end=\" \")\n",
        "    try:\n",
        "        t0 = time.perf_counter()\n",
        "        result = query_librarian(\n",
        "            entry['question'],\n",
        "            config_path=str(config_path),\n",
        "            generator_mode='openai',\n",
        "            verbose=False,\n",
        "        )\n",
        "        elapsed_ms = (time.perf_counter() - t0) * 1000.0\n",
        "\n",
        "        librarian_results.append({\n",
        "            'question': entry['question'],\n",
        "            'ground_truth': entry['answer'],\n",
        "            'prediction': result['answer'],\n",
        "            'latency_median_ms': round(elapsed_ms, 1),\n",
        "            'retrieval_ms': result['stats'].get('retrieval_ms'),\n",
        "            'generation_ms': result['stats'].get('generation_ms'),\n",
        "            'error': None,\n",
        "        })\n",
        "        print(f\"‚úì {elapsed_ms:.0f}ms\")\n",
        "    except Exception as e:\n",
        "        librarian_results.append({\n",
        "            'question': entry['question'],\n",
        "            'ground_truth': entry['answer'],\n",
        "            'prediction': f'[ERROR] {e}',\n",
        "            'latency_median_ms': None,\n",
        "            'retrieval_ms': None,\n",
        "            'generation_ms': None,\n",
        "            'error': str(e),\n",
        "        })\n",
        "        print(f\"‚úó {e}\")\n",
        "\n",
        "print(f\"\\n‚úì Librarian complete: {sum(1 for r in librarian_results if r['error'] is None)}/{len(sample)} succeeded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: ROUGE-L Scoring\n",
        "\n",
        "ROUGE-L measures the longest common subsequence between the predicted answer\n",
        "and the ground truth. Higher F1 = better textual overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìä Computing ROUGE-L scores...\\n\")\n",
        "\n",
        "for results_list, name in [(intern_results, 'Intern'), (librarian_results, 'Librarian')]:\n",
        "    for r in results_list:\n",
        "        if r['error'] is None:\n",
        "            rouge = compute_rouge_l(r['prediction'], r['ground_truth'])\n",
        "            r['rouge_l_precision'] = rouge['precision']\n",
        "            r['rouge_l_recall'] = rouge['recall']\n",
        "            r['rouge_l_f1'] = rouge['fmeasure']\n",
        "        else:\n",
        "            r['rouge_l_precision'] = None\n",
        "            r['rouge_l_recall'] = None\n",
        "            r['rouge_l_f1'] = None\n",
        "\n",
        "# Display ROUGE-L results side by side\n",
        "rouge_rows = []\n",
        "for i in range(len(sample)):\n",
        "    rouge_rows.append({\n",
        "        'Q#': i + 1,\n",
        "        'Question': sample[i]['question'][:60] + '...',\n",
        "        'Intern ROUGE-L F1': intern_results[i].get('rouge_l_f1'),\n",
        "        'Librarian ROUGE-L F1': librarian_results[i].get('rouge_l_f1'),\n",
        "    })\n",
        "\n",
        "rouge_df = pd.DataFrame(rouge_rows)\n",
        "print(rouge_df.to_string(index=False))\n",
        "\n",
        "# Averages\n",
        "intern_f1s = [r['rouge_l_f1'] for r in intern_results if r['rouge_l_f1'] is not None]\n",
        "lib_f1s = [r['rouge_l_f1'] for r in librarian_results if r['rouge_l_f1'] is not None]\n",
        "\n",
        "print(f\"\\n--- ROUGE-L F1 Averages ---\")\n",
        "if intern_f1s:\n",
        "    print(f\"  Intern    : {sum(intern_f1s)/len(intern_f1s):.4f} (n={len(intern_f1s)})\")\n",
        "else:\n",
        "    print(f\"  Intern    : N/A (skipped)\")\n",
        "print(f\"  Librarian : {sum(lib_f1s)/len(lib_f1s):.4f} (n={len(lib_f1s)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: LLM-as-a-Judge (Faithfulness & Accuracy)\n",
        "\n",
        "We use **o3-mini** (a reasoning model) to score each answer on:\n",
        "- **Faithfulness** (1-5): Does the answer only contain information supported by the ground truth?\n",
        "- **Accuracy** (1-5): How factually correct and complete is it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"‚öñÔ∏è  Running LLM-as-a-Judge ({judge_model})...\\n\")\n",
        "\n",
        "for results_list, name in [(intern_results, 'Intern'), (librarian_results, 'Librarian')]:\n",
        "    print(f\"--- Judging {name} answers ---\")\n",
        "    for i, r in enumerate(results_list):\n",
        "        if r['error'] is not None:\n",
        "            r['judge_faithfulness'] = None\n",
        "            r['judge_accuracy'] = None\n",
        "            r['judge_reasoning'] = 'Skipped (answer not available)'\n",
        "            print(f\"  [{i+1}] SKIPPED\")\n",
        "            continue\n",
        "\n",
        "        print(f\"  [{i+1}/{len(results_list)}] Judging...\", end=\" \")\n",
        "        scores = llm_judge_score(\n",
        "            question=r['question'],\n",
        "            ground_truth=r['ground_truth'],\n",
        "            prediction=r['prediction'],\n",
        "            config=config,\n",
        "        )\n",
        "        r['judge_faithfulness'] = scores['faithfulness']\n",
        "        r['judge_accuracy'] = scores['accuracy']\n",
        "        r['judge_reasoning'] = scores['reasoning']\n",
        "        print(f\"Faith={scores['faithfulness']} Acc={scores['accuracy']}\")\n",
        "    print()\n",
        "\n",
        "print(\"‚úì Judging complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Results Table & Visualization\n",
        "\n",
        "A comprehensive comparison of both systems across all metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build per-question results table\n",
        "detail_rows = []\n",
        "for i in range(len(sample)):\n",
        "    ir = intern_results[i]\n",
        "    lr = librarian_results[i]\n",
        "    detail_rows.append({\n",
        "        'Q#': i + 1,\n",
        "        'Category': sample[i]['category'],\n",
        "        'Question': sample[i]['question'][:50] + '...',\n",
        "        # Intern metrics\n",
        "        'Intern ROUGE-L': ir.get('rouge_l_f1'),\n",
        "        'Intern Faith.': ir.get('judge_faithfulness'),\n",
        "        'Intern Acc.': ir.get('judge_accuracy'),\n",
        "        'Intern Latency (ms)': ir.get('latency_median_ms'),\n",
        "        # Librarian metrics\n",
        "        'Lib. ROUGE-L': lr.get('rouge_l_f1'),\n",
        "        'Lib. Faith.': lr.get('judge_faithfulness'),\n",
        "        'Lib. Acc.': lr.get('judge_accuracy'),\n",
        "        'Lib. Latency (ms)': lr.get('latency_median_ms'),\n",
        "    })\n",
        "\n",
        "detail_df = pd.DataFrame(detail_rows)\n",
        "print(\"üìã Per-Question Results\")\n",
        "print(\"=\" * 120)\n",
        "detail_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate summary\n",
        "def safe_mean(values):\n",
        "    valid = [v for v in values if v is not None]\n",
        "    return round(sum(valid) / len(valid), 4) if valid else None\n",
        "\n",
        "def safe_median(values):\n",
        "    import statistics\n",
        "    valid = [v for v in values if v is not None]\n",
        "    return round(statistics.median(valid), 1) if valid else None\n",
        "\n",
        "summary = {\n",
        "    'Metric': [\n",
        "        'ROUGE-L F1 (mean)',\n",
        "        'Faithfulness (mean, 1-5)',\n",
        "        'Accuracy (mean, 1-5)',\n",
        "        'Latency ‚Äì median (ms)',\n",
        "        'Latency ‚Äì min (ms)',\n",
        "        'Latency ‚Äì max (ms)',\n",
        "        'Success Rate',\n",
        "    ],\n",
        "    'The Intern': [\n",
        "        safe_mean([r['rouge_l_f1'] for r in intern_results]),\n",
        "        safe_mean([r.get('judge_faithfulness') for r in intern_results]),\n",
        "        safe_mean([r.get('judge_accuracy') for r in intern_results]),\n",
        "        safe_median([r['latency_median_ms'] for r in intern_results]),\n",
        "        safe_median([r['latency_min_ms'] for r in intern_results]) if intern_available else None,\n",
        "        safe_median([r['latency_max_ms'] for r in intern_results]) if intern_available else None,\n",
        "        f\"{sum(1 for r in intern_results if r['error'] is None)}/{len(intern_results)}\",\n",
        "    ],\n",
        "    'The Librarian': [\n",
        "        safe_mean([r['rouge_l_f1'] for r in librarian_results]),\n",
        "        safe_mean([r.get('judge_faithfulness') for r in librarian_results]),\n",
        "        safe_mean([r.get('judge_accuracy') for r in librarian_results]),\n",
        "        safe_median([r['latency_median_ms'] for r in librarian_results]),\n",
        "        safe_median([r.get('latency_min_ms') or r.get('latency_median_ms') for r in librarian_results]),\n",
        "        safe_median([r.get('latency_max_ms') or r.get('latency_median_ms') for r in librarian_results]),\n",
        "        f\"{sum(1 for r in librarian_results if r['error'] is None)}/{len(librarian_results)}\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(summary)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä AGGREGATE RESULTS: The Intern vs The Librarian\")\n",
        "print(\"=\" * 70)\n",
        "summary_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: side-by-side bar charts\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "colors_intern = '#4A90D9'\n",
        "colors_librarian = '#E67E22'\n",
        "\n",
        "# --- ROUGE-L F1 ---\n",
        "ax = axes[0]\n",
        "intern_rouge = [r.get('rouge_l_f1') or 0 for r in intern_results]\n",
        "lib_rouge = [r.get('rouge_l_f1') or 0 for r in librarian_results]\n",
        "x = range(1, len(sample) + 1)\n",
        "ax.bar([i - bar_width/2 for i in x], intern_rouge, bar_width, label='Intern', color=colors_intern, alpha=0.85)\n",
        "ax.bar([i + bar_width/2 for i in x], lib_rouge, bar_width, label='Librarian', color=colors_librarian, alpha=0.85)\n",
        "ax.set_xlabel('Question #')\n",
        "ax.set_ylabel('ROUGE-L F1')\n",
        "ax.set_title('ROUGE-L F1 per Question')\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "# --- Faithfulness ---\n",
        "ax = axes[1]\n",
        "intern_faith = [r.get('judge_faithfulness') or 0 for r in intern_results]\n",
        "lib_faith = [r.get('judge_faithfulness') or 0 for r in librarian_results]\n",
        "ax.bar([i - bar_width/2 for i in x], intern_faith, bar_width, label='Intern', color=colors_intern, alpha=0.85)\n",
        "ax.bar([i + bar_width/2 for i in x], lib_faith, bar_width, label='Librarian', color=colors_librarian, alpha=0.85)\n",
        "ax.set_xlabel('Question #')\n",
        "ax.set_ylabel('Score (1-5)')\n",
        "ax.set_title('Faithfulness (LLM Judge)')\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 5.5)\n",
        "\n",
        "# --- Accuracy ---\n",
        "ax = axes[2]\n",
        "intern_acc = [r.get('judge_accuracy') or 0 for r in intern_results]\n",
        "lib_acc = [r.get('judge_accuracy') or 0 for r in librarian_results]\n",
        "ax.bar([i - bar_width/2 for i in x], intern_acc, bar_width, label='Intern', color=colors_intern, alpha=0.85)\n",
        "ax.bar([i + bar_width/2 for i in x], lib_acc, bar_width, label='Librarian', color=colors_librarian, alpha=0.85)\n",
        "ax.set_xlabel('Question #')\n",
        "ax.set_ylabel('Score (1-5)')\n",
        "ax.set_title('Accuracy (LLM Judge)')\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 5.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('The Intern vs The Librarian ‚Äî Per-Question Comparison', y=1.03, fontsize=14, fontweight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate bar chart\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "metrics_agg = {\n",
        "    'ROUGE-L F1': (\n",
        "        safe_mean([r.get('rouge_l_f1') for r in intern_results]) or 0,\n",
        "        safe_mean([r.get('rouge_l_f1') for r in librarian_results]) or 0,\n",
        "    ),\n",
        "    'Faithfulness': (\n",
        "        safe_mean([r.get('judge_faithfulness') for r in intern_results]) or 0,\n",
        "        safe_mean([r.get('judge_faithfulness') for r in librarian_results]) or 0,\n",
        "    ),\n",
        "    'Accuracy': (\n",
        "        safe_mean([r.get('judge_accuracy') for r in intern_results]) or 0,\n",
        "        safe_mean([r.get('judge_accuracy') for r in librarian_results]) or 0,\n",
        "    ),\n",
        "}\n",
        "\n",
        "for ax, (metric, (intern_val, lib_val)) in zip(axes, metrics_agg.items()):\n",
        "    bars = ax.bar(\n",
        "        ['Intern', 'Librarian'],\n",
        "        [intern_val, lib_val],\n",
        "        color=[colors_intern, colors_librarian],\n",
        "        alpha=0.85,\n",
        "        edgecolor='white',\n",
        "    )\n",
        "    ax.set_title(metric, fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Score')\n",
        "    for bar, val in zip(bars, [intern_val, lib_val]):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "                f'{val:.3f}' if val < 1 else f'{val:.2f}',\n",
        "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
        "    if metric == 'ROUGE-L F1':\n",
        "        ax.set_ylim(0, 1.1)\n",
        "    else:\n",
        "        ax.set_ylim(0, 5.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Aggregate Comparison', y=1.03, fontsize=14, fontweight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Bonus ‚Äî Cost Analysis\n",
        "\n",
        "Estimate monthly cloud cost for both strategies at scale:\n",
        "- **500 daily users** x **10 queries each** = **150,000 queries/month**\n",
        "\n",
        "| Strategy | Infrastructure | API Cost |\n",
        "|----------|---------------|----------|\n",
        "| Intern | AWS g4dn.xlarge (24/7) | None (self-hosted) |\n",
        "| Librarian | Lightweight instance + Weaviate | OpenAI gpt-4o-mini API |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cost = estimate_monthly_cost(config)\n",
        "\n",
        "print(\"üí∞ Monthly Cost Estimation\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Scale: {cost['queries_per_month']:,} queries/month\")\n",
        "print(f\"       ({config['cost_analysis']['users_per_day']} users √ó \"\n",
        "      f\"{config['cost_analysis']['queries_per_user']} queries √ó \"\n",
        "      f\"{config['cost_analysis']['days_per_month']} days)\")\n",
        "\n",
        "print(f\"\\n--- The Intern (Self-Hosted) ---\")\n",
        "ic = cost['intern']\n",
        "print(f\"  Instance     : {ic['instance_type']}\")\n",
        "print(f\"  Hourly cost  : ${ic['hourly_cost_usd']:.3f}\")\n",
        "print(f\"  Hours/month  : {ic['hours_per_month']}\")\n",
        "print(f\"  Compute cost : ${ic['compute_cost_usd']:,.2f}\")\n",
        "print(f\"  API cost     : ${ic['api_cost_usd']:.2f}\")\n",
        "print(f\"  ‚ñ∫ TOTAL      : ${ic['total_monthly_usd']:,.2f}/month\")\n",
        "\n",
        "print(f\"\\n--- The Librarian (API-Based RAG) ---\")\n",
        "lc = cost['librarian']\n",
        "print(f\"  LLM model    : {lc['llm_model']}\")\n",
        "print(f\"  Input tokens  : {lc['input_tokens_total']:,} (${lc['api_input_cost_usd']:.2f})\")\n",
        "print(f\"  Output tokens : {lc['output_tokens_total']:,} (${lc['api_output_cost_usd']:.2f})\")\n",
        "print(f\"  API total     : ${lc['api_total_usd']:.2f}\")\n",
        "print(f\"  Compute       : {lc['compute_instance']} ‚Üí ${lc['compute_cost_usd']:,.2f}\")\n",
        "print(f\"  ‚ñ∫ TOTAL       : ${lc['total_monthly_usd']:,.2f}/month\")\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(f\"üí° Cheaper option: {cost['summary']['cheaper']} \"\n",
        "      f\"(saves ~{cost['summary']['savings_pct']}%)\")\n",
        "print(f\"\\n‚ö†Ô∏è  {cost['summary']['disclaimer']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cost comparison chart\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "strategies = ['The Intern\\n(Self-Hosted)', 'The Librarian\\n(API + RAG)']\n",
        "\n",
        "# Stacked bar: compute + API\n",
        "compute_costs = [cost['intern']['compute_cost_usd'], cost['librarian']['compute_cost_usd']]\n",
        "api_costs = [cost['intern']['api_cost_usd'], cost['librarian']['api_total_usd']]\n",
        "totals = [cost['intern']['total_monthly_usd'], cost['librarian']['total_monthly_usd']]\n",
        "\n",
        "bars1 = ax.bar(strategies, compute_costs, label='Compute (GPU/CPU)', color=colors_intern, alpha=0.85)\n",
        "bars2 = ax.bar(strategies, api_costs, bottom=compute_costs, label='API Cost', color=colors_librarian, alpha=0.85)\n",
        "\n",
        "for bar, total in zip(bars1, totals):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, total + 5,\n",
        "            f'${total:,.0f}', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
        "\n",
        "ax.set_ylabel('Monthly Cost (USD)')\n",
        "ax.set_title('Monthly Cost at Scale (150K queries/month)', fontweight='bold')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What We Evaluated\n",
        "\n",
        "| Component | Implementation |\n",
        "|-----------|---------------|\n",
        "| **Golden Test Set** | 20 stratified-sampled questions from 1,008 total |\n",
        "| **System A ‚Äì The Intern** | Fine-tuned Llama-3-8B with LoRA adapters (Part 2) |\n",
        "| **System B ‚Äì The Librarian** | Hybrid RAG (Dense+BM25 ‚Üí RRF ‚Üí Rerank) + gpt-4o-mini (Part 3) |\n",
        "| **ROUGE-L** | Textual overlap with ground truth (precision, recall, F1) |\n",
        "| **LLM-as-a-Judge** | o3-mini scoring Faithfulness & Accuracy (1-5 scale) |\n",
        "| **Latency** | End-to-end response time in milliseconds |\n",
        "| **Cost Analysis** | Monthly projection for 500 users √ó 10 queries/day |\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Accuracy vs Cost Trade-off**: The Librarian (RAG + gpt-4o-mini) typically\n",
        "   achieves higher accuracy on factual questions because it retrieves fresh context\n",
        "   from the document. The Intern relies on what it learned during fine-tuning.\n",
        "\n",
        "2. **Faithfulness**: RAG-based systems tend to score higher on faithfulness because\n",
        "   the answer is grounded in retrieved passages. Fine-tuned models may hallucinate\n",
        "   when the question falls outside their training distribution.\n",
        "\n",
        "3. **Latency**: The Intern runs on a local GPU and is typically faster per query\n",
        "   (no network round-trip). The Librarian adds retrieval + API latency.\n",
        "\n",
        "4. **Cost at Scale**: Self-hosting (Intern) has a fixed compute cost regardless\n",
        "   of query volume. The Librarian's API cost scales linearly but may still be\n",
        "   cheaper at moderate volumes due to low per-token pricing of gpt-4o-mini.\n",
        "\n",
        "### Artifacts\n",
        "\n",
        "- `src/evaluation/` ‚Äî reusable evaluation modules (metrics, cost analysis)\n",
        "- `config/config.yaml` ‚Äî evaluation parameters under `evaluation.*` and `cost_analysis.*`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
