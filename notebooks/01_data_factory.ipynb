{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3a7e78d5",
      "metadata": {
        "id": "3a7e78d5"
      },
      "source": [
        "## Setup: Imports and Environment Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "51823097",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51823097",
        "outputId": "8758af6d-9301-47c1-9ead-e9e9a70918de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Imports and environment setup complete\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Load environment variables from .env file\n",
        "#load_dotenv(project_root / '.env')\n",
        "\n",
        "# Set OpenAI API key from environment\n",
        "#os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', os.environ.get('OPENAI_API_KEY', ''))\n",
        "#if not os.environ.get('OPENAI_API_KEY'):\n",
        "    #raise ValueError(\"OPENAI_API_KEY not found in .env file or environment variables\")\n",
        "\n",
        "# Import project modules\n",
        "from src.utils.config_loader import load_config\n",
        "from src.ingestion.pdf_loader import load_pdf, clean_text\n",
        "from src.ingestion.chunker import chunk_text\n",
        "from src.ingestion.qa_generator import generate_qa_pairs\n",
        "from src.ingestion.dataset_writer import save_to_jsonl, split_dataset, filter_valid_pairs\n",
        "\n",
        "print(\"✓ Imports and environment setup complete\")\n",
        "#print(f\"✓ Project root: {project_root}\")\n",
        "#print(f\"✓ OpenAI API key loaded: {'Yes' if os.environ.get('OPENAI_API_KEY') else 'No'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0kn-2gAJMKJB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kn-2gAJMKJB",
        "outputId": "4022fc4a-ee9e-4323-c48e-fab78dbd43e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ OpenAI API key loaded and set as environment variable\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Retrieve API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Set environment variable for OpenAI client\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "print(\"✓ OpenAI API key loaded and set as environment variable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbc45725",
      "metadata": {
        "id": "bbc45725"
      },
      "source": [
        "## Step 1: Load Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "5e781d39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e781d39",
        "outputId": "f68c238b-58f8-446d-e153-fed698dc4708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Configuration loaded\n",
            "  Raw data directory: /content/operation-ledger-mind/data/raw (exists: True)\n",
            "  PDF path: /content/operation-ledger-mind/data/raw/2024-Annual-Report.pdf (exists: True)\n",
            "  Output path: /content/operation-ledger-mind/data/output\n",
            "  Chunk size: 1500 characters\n",
            "  Questions per chunk: 10\n",
            "  Categories: hard_facts, strategic_summary, stylistic_creative\n"
          ]
        }
      ],
      "source": [
        "# Load configuration\n",
        "config_path = project_root / 'config' / 'config.yaml'\n",
        "config = load_config(config_path)\n",
        "\n",
        "# Extract data factory configuration\n",
        "df_config = config['data_factory']\n",
        "ingestion_config = df_config['ingestion']\n",
        "chunking_config = df_config['chunking']\n",
        "generation_config = df_config['generation']\n",
        "dataset_config = df_config['dataset']\n",
        "\n",
        "# Get paths\n",
        "raw_data_path = project_root / config['environment']['paths']['raw_data']\n",
        "output_path = project_root / config['environment']['paths']['data_dir'] / 'output'\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Verify raw_data_path exists\n",
        "if not raw_data_path.exists():\n",
        "    raise FileNotFoundError(f\"Raw data directory not found: {raw_data_path}\")\n",
        "\n",
        "# Get PDF filename from config, with fallback to actual file\n",
        "pdf_filename = config['project']['document']\n",
        "pdf_path = raw_data_path / pdf_filename\n",
        "\n",
        "# If configured PDF doesn't exist, try to find any PDF in the directory\n",
        "if not pdf_path.exists():\n",
        "    pdf_files = list(raw_data_path.glob('*.pdf'))\n",
        "    if pdf_files:\n",
        "        pdf_path = pdf_files[0]\n",
        "        pdf_filename = pdf_path.name\n",
        "        print(f\"⚠ Config PDF '{config['project']['document']}' not found, using: {pdf_filename}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"PDF file not found in {raw_data_path}. Expected: {pdf_path}\")\n",
        "\n",
        "# Verify PDF exists\n",
        "if not pdf_path.exists():\n",
        "    raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
        "\n",
        "print(\"✓ Configuration loaded\")\n",
        "print(f\"  Raw data directory: {raw_data_path} (exists: {raw_data_path.exists()})\")\n",
        "print(f\"  PDF path: {pdf_path} (exists: {pdf_path.exists()})\")\n",
        "print(f\"  Output path: {output_path}\")\n",
        "print(f\"  Chunk size: {chunking_config['chunk_size']} characters\")\n",
        "print(f\"  Questions per chunk: {generation_config['questions_per_chunk']}\")\n",
        "print(f\"  Categories: {', '.join(generation_config['categories'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18158515",
      "metadata": {
        "id": "18158515"
      },
      "source": [
        "## Step 2: PDF Ingestion & Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9be6bf70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9be6bf70",
        "outputId": "19888ca0-47ca-43a9-bc16-806cc91fd6f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading PDF from: /content/operation-ledger-mind/data/raw/2024-Annual-Report.pdf\n",
            "Error loading PDF: Failed to extract text from PDF. Please install one of: PyMuPDF (fitz), pdfplumber, or PyPDF2. Attempting to install pdfplumber...\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.9-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20251230 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20251230-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-5.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251230->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251230->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.23)\n",
            "Downloading pdfplumber-0.11.9-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20251230-py3-none-any.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m142.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-5.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20251230 pdfplumber-0.11.9 pypdfium2-5.3.0\n",
            "✓ PDF loaded after installing pdfplumber: 620,266 characters\n",
            "\n",
            "Cleaning text...\n",
            "✓ Text cleaned: 619,806 characters\n",
            "  Reduction: 460 characters (0.1%\n"
          ]
        }
      ],
      "source": [
        "# Load PDF\n",
        "print(f\"Loading PDF from: {pdf_path}\")\n",
        "\n",
        "try:\n",
        "    raw_text = load_pdf(pdf_path)\n",
        "    print(f\"✓ PDF loaded: {len(raw_text):,} characters\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error loading PDF: {e}. Attempting to install pdfplumber...\")\n",
        "    %pip install pdfplumber  # Install pdfplumber if it's missing\n",
        "    raw_text = load_pdf(pdf_path) # Retry loading after installation\n",
        "    print(f\"✓ PDF loaded after installing pdfplumber: {len(raw_text):,} characters\")\n",
        "\n",
        "# Clean text\n",
        "print(\"\\nCleaning text...\")\n",
        "cleaned_text = clean_text(\n",
        "    raw_text,\n",
        "    remove_headers=ingestion_config['remove_headers'],\n",
        "    remove_footers=ingestion_config['remove_footers'],\n",
        "    normalize_whitespace=ingestion_config['normalize_whitespace']\n",
        ")\n",
        "print(f\"✓ Text cleaned: {len(cleaned_text):,} characters\")\n",
        "print(f\"  Reduction: {len(raw_text) - len(cleaned_text):,} characters ({100*(len(raw_text)-len(cleaned_text))/len(raw_text):.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41b59eed",
      "metadata": {
        "id": "41b59eed"
      },
      "source": [
        "## Step 3: Chunking Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e4cbdc3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4cbdc3d",
        "outputId": "ca13f941-8d2f-4639-8f3b-5647b002e049"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunking document...\n",
            "✓ Document chunked into 414 chunks\n",
            "  Average chunk size: 1497 characters\n",
            "  Total characters: 619,806\n",
            "\n",
            "First chunk preview (chunk_id=0):\n",
            "  On Our Way\n",
            "Uber’s Mission\n",
            "We reimagine the way the world moves for the better\n",
            "We are Uber. The go-getters. The kind of people who are relentless about our\n",
            "mission to help people go anywhere and get an...\n"
          ]
        }
      ],
      "source": [
        "# Chunk the document\n",
        "print(\"Chunking document...\")\n",
        "chunks = chunk_text(\n",
        "    cleaned_text,\n",
        "    chunk_size=chunking_config['chunk_size'],\n",
        "    overlap=chunking_config['overlap']\n",
        ")\n",
        "\n",
        "print(f\"✓ Document chunked into {len(chunks)} chunks\")\n",
        "print(f\"  Average chunk size: {sum(len(c['text']) for c in chunks) / len(chunks):.0f} characters\")\n",
        "print(f\"  Total characters: {sum(len(c['text']) for c in chunks):,}\")\n",
        "\n",
        "# Display first chunk preview\n",
        "if chunks:\n",
        "    print(f\"\\nFirst chunk preview (chunk_id={chunks[0]['chunk_id']}):\")\n",
        "    print(f\"  {chunks[0]['text'][:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "109b85a9",
      "metadata": {
        "id": "109b85a9"
      },
      "source": [
        "## Step 4: Q/A Generation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "620b34a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "620b34a8",
        "outputId": "89bb8cdd-a592-4c5e-e3f9-cee52f25035b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM Configuration:\n",
            "  Question LLM: gpt-4o-mini (temp=0.3)\n",
            "  Answer LLM: gpt-4o (temp=0.2)\n",
            "  Categories: hard_facts, strategic_summary, stylistic_creative\n"
          ]
        }
      ],
      "source": [
        "# Prepare LLM configurations\n",
        "question_llm_config = {\n",
        "    'provider': generation_config['question_llm']['provider'],\n",
        "    'model': generation_config['question_llm']['model'],\n",
        "    'temperature': generation_config['question_llm']['temperature'],\n",
        "    'max_tokens': generation_config['question_llm']['max_tokens'],\n",
        "    'timeout_seconds': config['providers']['openai']['timeout_seconds'],\n",
        "    'max_retries': config['providers']['openai']['max_retries']\n",
        "}\n",
        "\n",
        "answer_llm_config = {\n",
        "    'provider': generation_config['answer_llm']['provider'],\n",
        "    'model': generation_config['answer_llm']['model'],\n",
        "    'temperature': generation_config['answer_llm']['temperature'],\n",
        "    'max_tokens': generation_config['answer_llm']['max_tokens'],\n",
        "    'timeout_seconds': config['providers']['openai']['timeout_seconds'],\n",
        "    'max_retries': config['providers']['openai']['max_retries']\n",
        "}\n",
        "\n",
        "categories = generation_config['categories']\n",
        "\n",
        "print(\"LLM Configuration:\")\n",
        "print(f\"  Question LLM: {question_llm_config['model']} (temp={question_llm_config['temperature']})\")\n",
        "print(f\"  Answer LLM: {answer_llm_config['model']} (temp={answer_llm_config['temperature']})\")\n",
        "print(f\"  Categories: {', '.join(categories)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "faf19e5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faf19e5c",
        "outputId": "0574dd98-e4a0-49a6-ab81-753cea46b179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating Q/A pairs for 414 chunks...\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:   1%|          | 5/414 [00:44<54:59,  8.07s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 5/414 chunks - 50 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:   2%|▏         | 10/414 [01:24<53:06,  7.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 10/414 chunks - 100 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:   4%|▎         | 15/414 [02:07<59:24,  8.93s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 15/414 chunks - 150 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:   5%|▍         | 20/414 [02:44<51:08,  7.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 20/414 chunks - 200 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:   6%|▌         | 25/414 [03:36<1:10:15, 10.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 25/414 chunks - 250 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:   7%|▋         | 30/414 [04:17<58:41,  9.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 30/414 chunks - 300 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:   8%|▊         | 35/414 [05:03<58:55,  9.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 35/414 chunks - 350 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  10%|▉         | 40/414 [05:53<59:40,  9.57s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 40/414 chunks - 400 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  11%|█         | 45/414 [06:34<51:51,  8.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 45/414 chunks - 450 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  12%|█▏        | 50/414 [07:28<1:07:41, 11.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 50/414 chunks - 500 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  13%|█▎        | 55/414 [08:07<48:59,  8.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 55/414 chunks - 550 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  14%|█▍        | 60/414 [08:55<53:22,  9.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 60/414 chunks - 600 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  16%|█▌        | 65/414 [09:47<57:09,  9.83s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 65/414 chunks - 650 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  17%|█▋        | 70/414 [10:40<1:00:26, 10.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 70/414 chunks - 700 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  18%|█▊        | 75/414 [11:35<1:06:22, 11.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 75/414 chunks - 750 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  19%|█▉        | 80/414 [12:22<52:55,  9.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 80/414 chunks - 800 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  21%|██        | 85/414 [13:11<52:43,  9.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 85/414 chunks - 850 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  22%|██▏       | 90/414 [14:04<55:28, 10.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 90/414 chunks - 900 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  23%|██▎       | 95/414 [14:47<46:21,  8.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 95/414 chunks - 950 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  24%|██▍       | 100/414 [15:32<49:58,  9.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 100/414 chunks - 1000 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  25%|██▌       | 105/414 [16:17<48:49,  9.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 105/414 chunks - 1050 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  27%|██▋       | 110/414 [17:02<46:36,  9.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 110/414 chunks - 1100 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  28%|██▊       | 115/414 [17:59<53:09, 10.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 115/414 chunks - 1150 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  29%|██▉       | 120/414 [18:50<45:26,  9.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 120/414 chunks - 1200 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  30%|███       | 125/414 [19:48<53:08, 11.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 125/414 chunks - 1250 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  31%|███▏      | 130/414 [20:32<42:52,  9.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 130/414 chunks - 1300 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  33%|███▎      | 135/414 [21:13<37:32,  8.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 135/414 chunks - 1350 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  34%|███▍      | 140/414 [21:58<42:12,  9.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 140/414 chunks - 1400 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  35%|███▌      | 145/414 [22:39<36:08,  8.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 145/414 chunks - 1450 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  36%|███▌      | 150/414 [23:17<35:01,  7.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 150/414 chunks - 1500 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  37%|███▋      | 155/414 [23:55<31:15,  7.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 155/414 chunks - 1550 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  39%|███▊      | 160/414 [24:46<39:59,  9.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 160/414 chunks - 1600 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  40%|███▉      | 165/414 [25:34<40:14,  9.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 165/414 chunks - 1650 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  41%|████      | 170/414 [26:31<50:30, 12.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 170/414 chunks - 1700 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  42%|████▏     | 175/414 [27:18<40:36, 10.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 175/414 chunks - 1750 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  43%|████▎     | 180/414 [27:57<31:57,  8.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 180/414 chunks - 1800 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  45%|████▍     | 185/414 [28:48<32:34,  8.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 185/414 chunks - 1850 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  46%|████▌     | 190/414 [29:24<27:47,  7.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 190/414 chunks - 1900 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  47%|████▋     | 195/414 [29:59<24:58,  6.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 195/414 chunks - 1950 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  48%|████▊     | 200/414 [30:35<24:17,  6.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 200/414 chunks - 2000 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  50%|████▉     | 205/414 [31:15<28:07,  8.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 205/414 chunks - 2050 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  51%|█████     | 210/414 [31:52<24:01,  7.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 210/414 chunks - 2100 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  52%|█████▏    | 215/414 [32:33<26:05,  7.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 215/414 chunks - 2150 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  53%|█████▎    | 220/414 [33:07<21:51,  6.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 220/414 chunks - 2200 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  54%|█████▍    | 225/414 [33:52<29:04,  9.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 225/414 chunks - 2250 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  56%|█████▌    | 230/414 [34:41<29:18,  9.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 230/414 chunks - 2300 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  57%|█████▋    | 235/414 [35:33<30:38, 10.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 235/414 chunks - 2350 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  58%|█████▊    | 240/414 [36:20<26:07,  9.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 240/414 chunks - 2400 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  59%|█████▉    | 245/414 [37:09<27:49,  9.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 245/414 chunks - 2450 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  60%|██████    | 250/414 [37:48<21:52,  8.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 250/414 chunks - 2500 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  62%|██████▏   | 255/414 [38:32<23:21,  8.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 255/414 chunks - 2550 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  63%|██████▎   | 260/414 [39:12<21:20,  8.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 260/414 chunks - 2600 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  64%|██████▍   | 265/414 [39:53<20:16,  8.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 265/414 chunks - 2650 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  65%|██████▌   | 270/414 [40:37<20:06,  8.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 270/414 chunks - 2700 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  66%|██████▋   | 275/414 [41:24<21:54,  9.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 275/414 chunks - 2750 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  68%|██████▊   | 280/414 [41:58<16:08,  7.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 280/414 chunks - 2800 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  69%|██████▉   | 285/414 [42:41<16:46,  7.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 285/414 chunks - 2850 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  70%|███████   | 290/414 [43:25<19:48,  9.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 290/414 chunks - 2900 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  71%|███████▏  | 295/414 [44:06<15:54,  8.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 295/414 chunks - 2950 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  72%|███████▏  | 300/414 [44:46<15:00,  7.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 300/414 chunks - 3000 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  74%|███████▎  | 305/414 [45:31<15:57,  8.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 305/414 chunks - 3050 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  75%|███████▍  | 310/414 [46:12<14:30,  8.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 310/414 chunks - 3100 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  76%|███████▌  | 315/414 [46:55<13:28,  8.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 315/414 chunks - 3150 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  77%|███████▋  | 320/414 [47:42<14:04,  8.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 320/414 chunks - 3200 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  79%|███████▊  | 325/414 [48:23<12:28,  8.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 325/414 chunks - 3250 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  80%|███████▉  | 330/414 [48:57<09:54,  7.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 330/414 chunks - 3300 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  81%|████████  | 335/414 [49:27<08:09,  6.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 335/414 chunks - 3350 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  82%|████████▏ | 340/414 [50:06<10:03,  8.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 340/414 chunks - 3400 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  83%|████████▎ | 345/414 [50:44<08:24,  7.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 345/414 chunks - 3450 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  85%|████████▍ | 350/414 [51:20<07:20,  6.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 350/414 chunks - 3500 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  86%|████████▌ | 355/414 [51:58<07:06,  7.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 355/414 chunks - 3550 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  87%|████████▋ | 360/414 [52:45<08:36,  9.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 360/414 chunks - 3600 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  88%|████████▊ | 365/414 [53:28<07:34,  9.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 365/414 chunks - 3650 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  89%|████████▉ | 370/414 [54:09<06:13,  8.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 370/414 chunks - 3700 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  91%|█████████ | 375/414 [54:45<04:39,  7.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 375/414 chunks - 3750 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  92%|█████████▏| 380/414 [55:28<04:25,  7.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 380/414 chunks - 3800 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  93%|█████████▎| 385/414 [56:08<03:44,  7.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 385/414 chunks - 3850 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  94%|█████████▍| 390/414 [56:45<02:50,  7.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 390/414 chunks - 3900 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  95%|█████████▌| 395/414 [57:18<02:13,  7.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 395/414 chunks - 3950 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  97%|█████████▋| 400/414 [57:54<01:42,  7.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 400/414 chunks - 4000 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  98%|█████████▊| 405/414 [58:27<00:58,  6.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 405/414 chunks - 4050 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks:  99%|█████████▉| 410/414 [58:56<00:23,  5.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 410/414 chunks - 4100 Q/A pairs generated\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks: 100%|██████████| 414/414 [59:21<00:00,  8.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "\n",
            "✓ Q/A generation complete!\n",
            "  Total Q/A pairs: 4140\n",
            "  Expected: 4140\n",
            "  Failed chunks: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate Q/A pairs for each chunk\n",
        "all_qa_pairs = []\n",
        "failed_chunks = []\n",
        "\n",
        "print(f\"\\nGenerating Q/A pairs for {len(chunks)} chunks...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for chunk in tqdm(chunks, desc=\"Processing chunks\"):\n",
        "    chunk_id = chunk['chunk_id']\n",
        "\n",
        "    try:\n",
        "        # Generate Q/A pairs for this chunk\n",
        "        qa_pairs = generate_qa_pairs(\n",
        "            chunk=chunk,\n",
        "            question_llm_config=question_llm_config,\n",
        "            answer_llm_config=answer_llm_config,\n",
        "            categories=categories\n",
        "        )\n",
        "\n",
        "        # Add chunk text reference to each pair (optional, for debugging)\n",
        "        for pair in qa_pairs:\n",
        "            pair['chunk_text'] = chunk['text'][:500]  # Store first 500 chars for reference\n",
        "\n",
        "        all_qa_pairs.extend(qa_pairs)\n",
        "\n",
        "        # Progress update\n",
        "        if (chunk_id + 1) % 5 == 0:\n",
        "            print(f\"  Processed {chunk_id + 1}/{len(chunks)} chunks - {len(all_qa_pairs)} Q/A pairs generated\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n⚠ Error processing chunk {chunk_id}: {str(e)}\")\n",
        "        failed_chunks.append(chunk_id)\n",
        "        continue\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n✓ Q/A generation complete!\")\n",
        "print(f\"  Total Q/A pairs: {len(all_qa_pairs)}\")\n",
        "print(f\"  Expected: {len(chunks) * generation_config['questions_per_chunk']}\")\n",
        "print(f\"  Failed chunks: {len(failed_chunks)}\")\n",
        "if failed_chunks:\n",
        "    print(f\"  Failed chunk IDs: {failed_chunks}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a202df7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a202df7e",
        "outputId": "66ab5b7b-932c-4489-b552-6417ce053cd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quality Check:\n",
            "  Valid pairs: 4140\n",
            "  Invalid pairs removed: 0\n",
            "\n",
            "Category Distribution:\n",
            "  strategic_summary: 708 (17.1%)\n",
            "  hard_facts: 2533 (61.2%)\n",
            "  stylistic_creative: 899 (21.7%)\n",
            "\n",
            "Sample Q/A Pairs:\n",
            "\n",
            "  Example 1 (strategic_summary):\n",
            "    Q: What is Uber's mission as stated in the annual report?...\n",
            "    A: We reimagine the way the world moves for the better...\n",
            "\n",
            "  Example 2 (hard_facts):\n",
            "    Q: For which fiscal year does this annual report apply?...\n",
            "    A: For the fiscal year ended December 31, 2024...\n",
            "\n",
            "  Example 3 (hard_facts):\n",
            "    Q: What is the commission file number associated with Uber Technologies, Inc.?...\n",
            "    A: 001-38902...\n"
          ]
        }
      ],
      "source": [
        "# Filter out invalid pairs and show statistics\n",
        "valid_pairs = filter_valid_pairs(all_qa_pairs)\n",
        "invalid_count = len(all_qa_pairs) - len(valid_pairs)\n",
        "\n",
        "print(f\"Quality Check:\")\n",
        "print(f\"  Valid pairs: {len(valid_pairs)}\")\n",
        "print(f\"  Invalid pairs removed: {invalid_count}\")\n",
        "\n",
        "# Category distribution\n",
        "category_counts = {}\n",
        "for pair in valid_pairs:\n",
        "    category = pair.get('category', 'unknown')\n",
        "    category_counts[category] = category_counts.get(category, 0) + 1\n",
        "\n",
        "print(f\"\\nCategory Distribution:\")\n",
        "for category, count in category_counts.items():\n",
        "    percentage = 100 * count / len(valid_pairs) if valid_pairs else 0\n",
        "    print(f\"  {category}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "# Display sample Q/A pairs\n",
        "if valid_pairs:\n",
        "    print(f\"\\nSample Q/A Pairs:\")\n",
        "    for i, pair in enumerate(valid_pairs[:3], 1):\n",
        "        print(f\"\\n  Example {i} ({pair.get('category', 'unknown')}):\")\n",
        "        print(f\"    Q: {pair['question'][:100]}...\")\n",
        "        print(f\"    A: {pair['answer'][:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d50ccc8",
      "metadata": {
        "id": "6d50ccc8"
      },
      "source": [
        "## Step 5: Dataset Splitting & Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "972ed4ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "972ed4ee",
        "outputId": "8653e434-ba81-42cd-9a91-9df2a142b492"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splitting dataset...\n",
            "✓ Dataset split:\n",
            "  Train set: 3312 pairs (80.0%)\n",
            "  Test set: 828 pairs (20.0%)\n"
          ]
        }
      ],
      "source": [
        "# Split dataset into train and test sets\n",
        "print(\"Splitting dataset...\")\n",
        "train_pairs, test_pairs = split_dataset(\n",
        "    all_pairs=valid_pairs,\n",
        "    train_split=dataset_config['train_split'],\n",
        "    shuffle=dataset_config['shuffle_before_split'],\n",
        "    seed=dataset_config['seed']\n",
        ")\n",
        "\n",
        "print(f\"✓ Dataset split:\")\n",
        "print(f\"  Train set: {len(train_pairs)} pairs ({100*len(train_pairs)/len(valid_pairs):.1f}%)\")\n",
        "print(f\"  Test set: {len(test_pairs)} pairs ({100*len(test_pairs)/len(valid_pairs):.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2151acfe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2151acfe",
        "outputId": "21964a1f-9c8b-4ebf-c1cd-2d3c0df915ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saving datasets...\n",
            "✓ Datasets saved:\n",
            "  Train: /content/operation-ledger-mind/data/output/train.jsonl\n",
            "  Test: /content/operation-ledger-mind/data/output/golden_test_set.jsonl\n",
            "\n",
            "File verification:\n",
            "  Train file size: 2620.1 KB\n",
            "  Test file size: 659.1 KB\n"
          ]
        }
      ],
      "source": [
        "# Save datasets to JSONL files\n",
        "train_file = output_path / dataset_config['train_file']\n",
        "test_file = output_path / dataset_config['test_file']\n",
        "\n",
        "print(f\"\\nSaving datasets...\")\n",
        "save_to_jsonl(train_pairs, train_file)\n",
        "save_to_jsonl(test_pairs, test_file)\n",
        "\n",
        "print(f\"✓ Datasets saved:\")\n",
        "print(f\"  Train: {train_file}\")\n",
        "print(f\"  Test: {test_file}\")\n",
        "\n",
        "# Verify files\n",
        "print(f\"\\nFile verification:\")\n",
        "print(f\"  Train file size: {train_file.stat().st_size / 1024:.1f} KB\")\n",
        "print(f\"  Test file size: {test_file.stat().st_size / 1024:.1f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d39cff7",
      "metadata": {
        "id": "1d39cff7"
      },
      "source": [
        "## Step 6: Verification & Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "fabdaa49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fabdaa49",
        "outputId": "5d9626ef-7596-445e-e3df-41a0413b8e3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verifying saved files...\n",
            "\n",
            "Train file samples (first 3):\n",
            "  1. Q: What valuation model is used to determine the grant-date fair value of market-ba...\n",
            "     Category: strategic_summary\n",
            "  2. Q: What is the tone of the report regarding the urgency of addressing climate chang...\n",
            "     Category: stylistic_creative\n",
            "  3. Q: What does Adjusted EBITDA represent in the context of the company's financial me...\n",
            "     Category: hard_facts\n",
            "\n",
            "Test file samples (first 3):\n",
            "  1. Q: In what ways could the potential disclosure of personal data impact the company'...\n",
            "     Category: hard_facts\n",
            "  2. Q: What types of assets are considered when allocating the fair value of purchase c...\n",
            "     Category: hard_facts\n",
            "  3. Q: How does the company plan to evaluate the impact of the new accounting standards...\n",
            "     Category: strategic_summary\n",
            "\n",
            "============================================================\n",
            "✓ DATA FACTORY PIPELINE COMPLETE!\n",
            "============================================================\n",
            "\n",
            "Summary:\n",
            "  • PDF processed: 2024-Annual-Report.pdf\n",
            "  • Chunks created: 414\n",
            "  • Total Q/A pairs: 4140\n",
            "  • Train set: 3312 pairs\n",
            "  • Test set: 828 pairs\n",
            "  • Output directory: /content/operation-ledger-mind/data/output\n"
          ]
        }
      ],
      "source": [
        "# Load and verify a few samples from each file\n",
        "print(\"Verifying saved files...\\n\")\n",
        "\n",
        "# Check train file\n",
        "with open(train_file, 'r', encoding='utf-8') as f:\n",
        "    train_samples = [json.loads(line) for line in f.readlines()[:3]]\n",
        "\n",
        "print(f\"Train file samples (first 3):\")\n",
        "for i, sample in enumerate(train_samples, 1):\n",
        "    print(f\"  {i}. Q: {sample['question'][:80]}...\")\n",
        "    print(f\"     Category: {sample.get('category', 'N/A')}\")\n",
        "\n",
        "# Check test file\n",
        "with open(test_file, 'r', encoding='utf-8') as f:\n",
        "    test_samples = [json.loads(line) for line in f.readlines()[:3]]\n",
        "\n",
        "print(f\"\\nTest file samples (first 3):\")\n",
        "for i, sample in enumerate(test_samples, 1):\n",
        "    print(f\"  {i}. Q: {sample['question'][:80]}...\")\n",
        "    print(f\"     Category: {sample.get('category', 'N/A')}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✓ DATA FACTORY PIPELINE COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  • PDF processed: {pdf_filename}\")\n",
        "print(f\"  • Chunks created: {len(chunks)}\")\n",
        "print(f\"  • Total Q/A pairs: {len(valid_pairs)}\")\n",
        "print(f\"  • Train set: {len(train_pairs)} pairs\")\n",
        "print(f\"  • Test set: {len(test_pairs)} pairs\")\n",
        "print(f\"  • Output directory: {output_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
