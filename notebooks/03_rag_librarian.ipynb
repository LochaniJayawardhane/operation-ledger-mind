{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: \"The Librarian\" ‚Äì Advanced Hybrid RAG System\n",
        "\n",
        "**Goal:** Build an Advanced Hybrid RAG Pipeline that retrieves financial entities\n",
        "(e.g., \"Form 10-K\", \"$37B\") that pure semantic search often misses.\n",
        "\n",
        "**Pipeline:**\n",
        "```\n",
        "Question ‚Üí Dense (Weaviate nearVector) + BM25 (Weaviate keyword)\n",
        "         ‚Üí Reciprocal Rank Fusion (RRF)\n",
        "         ‚Üí Cross-Encoder Reranking\n",
        "         ‚Üí Answer Generation (OpenAI / Intern fine-tuned / Base model)\n",
        "```\n",
        "\n",
        "**Steps in this notebook:**\n",
        "1. Setup: imports and environment\n",
        "2. Load config + locate PDF\n",
        "3. Build / load Weaviate index\n",
        "4. Dense retrieval demo\n",
        "5. BM25 retrieval demo\n",
        "6. RRF fusion\n",
        "7. Cross-encoder reranking\n",
        "8. End-to-end `query_librarian()` demo\n",
        "9. Generator comparison: OpenAI vs Intern fine-tuned vs Base model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Imports and Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path (same pattern as other notebooks)\n",
        "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
        "sys.path.insert(0, str(project_root))\n",
        "os.chdir(project_root)\n",
        "\n",
        "print(f\"‚úì Project root: {project_root}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load OpenAI API key (Colab secrets or .env)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"‚úì OpenAI API key loaded from Colab secrets\")\n",
        "except Exception:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv(project_root / '.env')\n",
        "    print(\"‚úì Environment loaded from .env\")\n",
        "\n",
        "print(f\"‚úì OPENAI_API_KEY set: {'Yes' if os.environ.get('OPENAI_API_KEY') else 'No'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import project modules\n",
        "from src.utils.config_loader import load_config\n",
        "from src.ingestion.pdf_loader import load_pdf, clean_text\n",
        "from src.ingestion.chunker import chunk_text\n",
        "\n",
        "# RAG modules\n",
        "from src.rag.weaviate_store import connect_weaviate, ensure_collection\n",
        "from src.rag.index_builder import ensure_index_built, build_index\n",
        "from src.rag.retrieval import dense_search, bm25_search\n",
        "from src.rag.fusion import rrf_fusion\n",
        "from src.rag.reranker import rerank\n",
        "from src.rag.generation import generate_answer, build_rag_prompt\n",
        "from src.rag.librarian_inference import query_librarian\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "\n",
        "print(\"‚úì All imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config_path = project_root / 'config' / 'config.yaml'\n",
        "config = load_config(config_path)\n",
        "\n",
        "rag_cfg = config['rag']\n",
        "\n",
        "# Resolve PDF path\n",
        "raw_data = project_root / config['environment']['paths']['raw_data']\n",
        "doc_name = config['project']['document']\n",
        "pdf_path = raw_data / doc_name\n",
        "\n",
        "print(\"‚úì Configuration loaded\")\n",
        "print(f\"  PDF: {pdf_path} (exists: {pdf_path.exists()})\")\n",
        "print(f\"  Vector DB: {rag_cfg['vector_db']['provider']} ({rag_cfg['vector_db']['mode']})\")\n",
        "print(f\"  Embedding model: {rag_cfg['embeddings']['model']}\")\n",
        "print(f\"  Reranker: {rag_cfg['refinement']['reranker']['model']}\")\n",
        "print(f\"  RRF k: {rag_cfg['refinement']['rrf']['k']}\")\n",
        "print(f\"  Retrieval top-k: {rag_cfg['retrieval']['top_k']}\")\n",
        "print(f\"  Reranker top-k: {rag_cfg['refinement']['reranker']['top_k']}\")\n",
        "print(f\"  Generator mode: {rag_cfg['inference'].get('generator_mode', 'openai')}\")\n",
        "print(f\"  Answer LLM: {rag_cfg['inference']['answer_llm']['model']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Build / Load Weaviate Index\n",
        "\n",
        "This loads the PDF, chunks it, embeds with SentenceTransformers, and upserts\n",
        "into Weaviate. Idempotent: if the index already exists, it skips rebuilding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the embedding model (cached for reuse)\n",
        "embedder = SentenceTransformer(rag_cfg['embeddings']['model'])\n",
        "print(f\"‚úì Embedder loaded: {rag_cfg['embeddings']['model']}\")\n",
        "print(f\"  Embedding dimension: {embedder.get_sentence_embedding_dimension()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to Weaviate and build/verify the index\n",
        "client = connect_weaviate(config)\n",
        "print(f\"‚úì Weaviate client connected ({rag_cfg['vector_db']['mode']} mode)\")\n",
        "\n",
        "count = ensure_index_built(\n",
        "    config,\n",
        "    client=client,\n",
        "    embedder=embedder,\n",
        "    force_rebuild=False,  # Set True to rebuild from scratch\n",
        "    verbose=True,\n",
        ")\n",
        "print(f\"\\n‚úì Weaviate index ready: {count} chunks indexed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Dense Retrieval Demo\n",
        "\n",
        "Dense (vector) retrieval encodes the question and finds chunks with the\n",
        "closest embedding vectors. Good for semantic/paraphrase queries but can\n",
        "miss exact entity names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_query = \"What was the total revenue reported in the annual report?\"\n",
        "\n",
        "dense_results = dense_search(\n",
        "    test_query,\n",
        "    client=client,\n",
        "    embedder=embedder,\n",
        "    config=config,\n",
        "    top_n=10,\n",
        ")\n",
        "\n",
        "print(f\"üîµ Dense search: '{test_query}'\")\n",
        "print(f\"   Retrieved {len(dense_results)} results\\n\")\n",
        "for i, r in enumerate(dense_results[:5], 1):\n",
        "    print(f\"  [{i}] chunk {r['meta']['chunk_id']} \"\n",
        "          f\"(score: {r['score']:.4f}) \"\n",
        "          f\"pp. {r['meta']['page_start']}-{r['meta']['page_end']}\")\n",
        "    print(f\"      {r['content'][:150]}...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: BM25 Retrieval Demo\n",
        "\n",
        "BM25 (keyword/sparse) retrieval matches exact terms. Essential for\n",
        "entity-heavy financial queries like \"Form 10-K\" or \"$37B\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bm25_results = bm25_search(\n",
        "    test_query,\n",
        "    client=client,\n",
        "    config=config,\n",
        "    top_n=10,\n",
        ")\n",
        "\n",
        "print(f\"üü† BM25 search: '{test_query}'\")\n",
        "print(f\"   Retrieved {len(bm25_results)} results\\n\")\n",
        "for i, r in enumerate(bm25_results[:5], 1):\n",
        "    print(f\"  [{i}] chunk {r['meta']['chunk_id']} \"\n",
        "          f\"(score: {r['score']:.4f}) \"\n",
        "          f\"pp. {r['meta']['page_start']}-{r['meta']['page_end']}\")\n",
        "    print(f\"      {r['content'][:150]}...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare: which chunks appear in dense but not BM25 and vice versa?\n",
        "dense_ids = {r['id'] for r in dense_results}\n",
        "bm25_ids = {r['id'] for r in bm25_results}\n",
        "\n",
        "print(f\"Dense-only chunks: {len(dense_ids - bm25_ids)}\")\n",
        "print(f\"BM25-only chunks:  {len(bm25_ids - dense_ids)}\")\n",
        "print(f\"Overlap:           {len(dense_ids & bm25_ids)}\")\n",
        "print(f\"\\n‚Üí This overlap gap is why hybrid search matters!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Reciprocal Rank Fusion (RRF)\n",
        "\n",
        "RRF combines the two ranked lists into one. Formula:\n",
        "\n",
        "$$\\text{RRF}(d) = \\sum_{i} \\frac{1}{k + \\text{rank}_i(d)}$$\n",
        "\n",
        "Docs that rank high in **both** lists get the highest fused scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rrf_k = int(config['rag']['refinement']['rrf']['k'])\n",
        "\n",
        "fused_results = rrf_fusion(dense_results, bm25_results, k=rrf_k)\n",
        "\n",
        "print(f\"üîÄ RRF Fusion (k={rrf_k}): {len(fused_results)} unique candidates\\n\")\n",
        "for i, r in enumerate(fused_results[:8], 1):\n",
        "    print(f\"  [{i}] chunk {r['meta']['chunk_id']} \"\n",
        "          f\"RRF={r['rrf_score']:.4f} \"\n",
        "          f\"(dense_rank={r.get('dense_rank', '-')}, bm25_rank={r.get('bm25_rank', '-')})\")\n",
        "    print(f\"      {r['content'][:120]}...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Cross-Encoder Reranking\n",
        "\n",
        "The cross-encoder sees (query, document) **together** ‚Äì unlike the bi-encoder\n",
        "which embeds them separately. This gives higher accuracy but is too slow\n",
        "for the full corpus, so we only apply it to the top fused candidates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load cross-encoder reranker\n",
        "reranker_model = CrossEncoder(rag_cfg['refinement']['reranker']['model'])\n",
        "rerank_top_k = int(rag_cfg['refinement']['reranker']['top_k'])\n",
        "\n",
        "print(f\"‚úì Reranker loaded: {rag_cfg['refinement']['reranker']['model']}\")\n",
        "\n",
        "reranked_results = rerank(\n",
        "    test_query,\n",
        "    fused_results[:20],  # rerank top-20 fused candidates\n",
        "    cross_encoder=reranker_model,\n",
        "    top_k=rerank_top_k,\n",
        ")\n",
        "\n",
        "print(f\"\\nüèÜ Reranked to top-{len(reranked_results)}:\\n\")\n",
        "for i, r in enumerate(reranked_results, 1):\n",
        "    print(f\"  [{i}] chunk {r['meta']['chunk_id']} \"\n",
        "          f\"rerank_score={r['rerank_score']:.3f} \"\n",
        "          f\"(RRF={r.get('rrf_score', 0):.4f})\")\n",
        "    print(f\"      {r['content'][:150]}...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: End-to-End `query_librarian()` Demo\n",
        "\n",
        "The `query_librarian(question)` function runs the full pipeline:\n",
        "Dense + BM25 ‚Üí RRF ‚Üí Rerank ‚Üí Generate answer.\n",
        "\n",
        "It returns the answer plus source chunks and pipeline statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Close the manually-opened client ‚Äì query_librarian manages its own\n",
        "client.close()\n",
        "print(\"‚úì Manual Weaviate client closed (query_librarian handles its own)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = query_librarian(\n",
        "    \"What was the total revenue reported in the annual report?\",\n",
        "    config_path=str(config_path),\n",
        "    generator_mode=\"openai\",\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"Answer ({result['generator_mode']}):\")\n",
        "print(\"=\" * 70)\n",
        "print(result['answer'])\n",
        "print(\"\\nPipeline stats:\")\n",
        "for k, v in result['stats'].items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(\"\\nTop sources:\")\n",
        "for i, src in enumerate(result['sources'], 1):\n",
        "    print(f\"  [{i}] chunk {src['chunk_id']} \"\n",
        "          f\"(pp. {src['page_start']}-{src['page_end']}) \"\n",
        "          f\"rerank={src['scores'].get('rerank', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Entity-Heavy Queries (Why Hybrid Search Matters)\n",
        "\n",
        "Financial documents contain specific entities that pure semantic search\n",
        "often misses. These queries test the hybrid retrieval advantage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "entity_queries = [\n",
        "    \"What information is disclosed in the Form 10-K filing?\",\n",
        "    \"What are the key financial metrics for fiscal year 2024?\",\n",
        "    \"What are the main business segments and their revenue contributions?\",\n",
        "    \"What risk factors are highlighted in the annual report?\",\n",
        "    \"What does the report say about stock-based compensation?\",\n",
        "]\n",
        "\n",
        "print(\"üìä Testing entity-heavy queries with query_librarian()\\n\")\n",
        "for q in entity_queries:\n",
        "    print(f\"Q: {q}\")\n",
        "    r = query_librarian(q, config_path=str(config_path), generator_mode=\"openai\", verbose=False)\n",
        "    print(f\"A: {r['answer'][:300]}{'...' if len(r['answer']) > 300 else ''}\")\n",
        "    print(f\"   [{r['stats']['total_ms']:.0f}ms | {r['stats']['reranked_k']} sources]\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Generator Comparison ‚Äì OpenAI vs Intern (Fine-tuned) vs Base Model\n",
        "\n",
        "**Experiment:** Run the **same retrieved context** through three different\n",
        "answer generators to compare how much fine-tuning impacts contextual\n",
        "understanding.\n",
        "\n",
        "| Generator | Description |\n",
        "|-----------|-------------|\n",
        "| **OpenAI (gpt-4o)** | Cloud API baseline ‚Äì strong general model |\n",
        "| **Intern fine-tuned** | Llama-3-8B + LoRA from Part 2 ‚Äì domain-adapted |\n",
        "| **Base model** | Llama-3-8B-Instruct without LoRA ‚Äì control group |\n",
        "\n",
        "> **Note:** Running Intern / Base requires GPU (Colab T4). If running on CPU-only,\n",
        "> this section will only show OpenAI results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "gpu_available = torch.cuda.is_available()\n",
        "print(f\"GPU available: {gpu_available}\")\n",
        "if gpu_available:\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Decide which generators to compare\n",
        "generators_to_test = [\"openai\"]\n",
        "if gpu_available:\n",
        "    generators_to_test.extend([\"intern_finetuned\", \"intern_base\"])\n",
        "    print(\"\\n‚úì Will compare: OpenAI vs Intern (fine-tuned) vs Base model\")\n",
        "else:\n",
        "    print(\"\\n‚ö† No GPU ‚Äì comparing OpenAI only. Run in Colab for full comparison.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comparison_questions = [\n",
        "    \"What was the total revenue reported in the annual report?\",\n",
        "    \"What are the main risk factors mentioned in the filing?\",\n",
        "    \"What is the company's strategy for growth in the coming years?\",\n",
        "]\n",
        "\n",
        "print(\"üî¨ Generator Comparison Experiment\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for q_idx, question in enumerate(comparison_questions, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Q{q_idx}: {question}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for mode in generators_to_test:\n",
        "        print(f\"\\n  ü§ñ Generator: {mode}\")\n",
        "        try:\n",
        "            t0 = time.time()\n",
        "            result = query_librarian(\n",
        "                question,\n",
        "                config_path=str(config_path),\n",
        "                generator_mode=mode,\n",
        "                verbose=False,\n",
        "            )\n",
        "            elapsed = (time.time() - t0) * 1000\n",
        "            print(f\"  Answer: {result['answer'][:400]}\")\n",
        "            print(f\"  ‚è± {elapsed:.0f}ms total\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ Generator comparison complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**What we built:**\n",
        "\n",
        "| Component | Implementation |\n",
        "|-----------|---------------|\n",
        "| **Vector Database** | Weaviate (embedded mode) |\n",
        "| **Dense Retrieval** | SentenceTransformer `all-MiniLM-L6-v2` ‚Üí Weaviate `near_vector` |\n",
        "| **Sparse Retrieval** | Weaviate built-in BM25 |\n",
        "| **Fusion** | Reciprocal Rank Fusion (RRF, k=60) |\n",
        "| **Reranking** | CrossEncoder `ms-marco-MiniLM-L-6-v2` |\n",
        "| **Generation** | OpenAI gpt-4o / Intern fine-tuned / Base model |\n",
        "| **Entrypoint** | `query_librarian(question)` ‚Üí answer + sources + stats |\n",
        "\n",
        "**Key Insights:**\n",
        "- Hybrid search (dense + BM25) catches entities that pure vector search misses\n",
        "- RRF elegantly combines rankings without needing score calibration\n",
        "- Cross-encoder reranking provides high-precision final ordering\n",
        "- Fine-tuning the generator on domain data can improve answer quality\n",
        "\n",
        "**Artifacts:**\n",
        "- `src/rag/` ‚Äì reusable RAG modules\n",
        "- `.weaviate/` ‚Äì embedded Weaviate persistence\n",
        "- `config/config.yaml` ‚Äì all parameters under `rag.*`"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
